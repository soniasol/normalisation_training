{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soniasol/test_normalisation_2/blob/main/notebooks/Train_norm_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIVO_5Oq3500"
      },
      "source": [
        "# Train a normaliser for French\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gabays/32M7131/blob/main/Cours_04/Cours04.ipynb)\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licence Creative Commons\" style=\"border-width:0;float:right;\\\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a>\n",
        "\n",
        "Simon Gabay (UniGE), Rachel Bawden (INRIA Paris)\n",
        "\n",
        "<img width=\"30px\" style=\"float:left\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"/>  Specific requirements for Colab users are signaled with this sign."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS9in_4_LZ97"
      },
      "source": [
        "## I. Preparing the experiment\n",
        "\n",
        "You might need first to clean your env (OOD user in Geneva)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**note**: bblabla"
      ],
      "metadata": {
        "id": "epurIP9mKdN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXFjCEFMKW9T"
      },
      "outputs": [],
      "source": [
        "!pip freeze --user --exclude-editable | xargs pip uninstall -y\n",
        "!pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD2irpZSKW9U"
      },
      "source": [
        "Installing required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG3o78iI30sR"
      },
      "outputs": [],
      "source": [
        "!pip install fairseq==0.12.2 sentencepiece sacrebleu omegaconf==2.0.5 gdown==4.2.0 tensorboardX numpy==1.25.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meKj3h29KW9U"
      },
      "source": [
        "### I.a Colab vs other tools\n",
        "\n",
        "<img width=\"30px\" style=\"float:left\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"/>  Colab users:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-cheDZ4KW9U"
      },
      "outputs": [],
      "source": [
        "DIRECTORY=\"/content/FreEMnorm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TCc3zHOKW9U"
      },
      "source": [
        "Non colab users:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5_HRJ3OKW9U"
      },
      "outputs": [],
      "source": [
        "DIRECTORY=\"FreEMnorm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZoNmqSlKhSu"
      },
      "source": [
        "### I.b Retrieving data\n",
        "We download the repo (`dev` branch only for now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79e9t2dvJWZe"
      },
      "outputs": [],
      "source": [
        "#delete if there is an older version of the repo\n",
        "!rm -rf $DIRECTORY\n",
        "# cloning dev branch\n",
        "!git clone -b dev https://github.com/FreEM-corpora/FreEMnorm.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gHRyxOoKted"
      },
      "source": [
        "### I.c If you want to redo the split and the creation of the training data\n",
        "\n",
        "We can split the corpus and create a file with the data in the requires format.\n",
        "\n",
        "<img width=\"30px\" style=\"float:left\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"/> Add `-c` to the last command (`split_to_src_trg.py`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzRsHEzd4WU3"
      },
      "outputs": [],
      "source": [
        "#Remove previously split directory if there is any\n",
        "!rm -rf $DIRECTORY/split/ $DIRECTORY/data/\n",
        "!echo \"Split and Data directories have been removed\"\n",
        "#We split the corpus into train/test/dev\n",
        "!python $DIRECTORY/split.py\n",
        "#We turn the split into the requires format\n",
        "!python $DIRECTORY/split_to_src_trg.py\n",
        "#Colab users should comment the previous one and use the following\n",
        "#!python /content/FreEMnorm/split_to_src_trg.py -c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLr0uJSPLmd7"
      },
      "source": [
        "## II. Traing a model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeQzsF3Lo9Qt"
      },
      "source": [
        "### II.a Preprocessing\n",
        "\n",
        "We will need a few functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H1PQJScKW9V"
      },
      "outputs": [],
      "source": [
        "# Read a file per line\n",
        "def read_file(filename):\n",
        "  list_sents = []\n",
        "  with open(filename) as fp:\n",
        "    for line in fp:\n",
        "      list_sents.append(line.strip())\n",
        "  return list_sents\n",
        "\n",
        "#write a file per line\n",
        "def write_file(list_sents, filename):\n",
        "    with open(filename, 'w') as fp:\n",
        "        for sent in list_sents:\n",
        "            fp.write(sent + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUDgRxGVKW9V"
      },
      "source": [
        "We will need various sizes of vocabulary: 2000, 3000, 4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIMXi--YLuHG"
      },
      "outputs": [],
      "source": [
        "import sentencepiece\n",
        "import os\n",
        "\n",
        "!rm -rf $DIRECTORY/data/vocabulary.src-trg\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_1000\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_2000\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_3000\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_4000\n",
        "\n",
        "# We make a big file with all the data\n",
        "!cat $DIRECTORY/data/* > $DIRECTORY/data/vocabulary.src-trg\n",
        "\n",
        "# 1000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_1000\"),\n",
        "                               vocab_size=1000)\n",
        "\n",
        "# 2000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_2000\"),\n",
        "                               vocab_size=2000)\n",
        "\n",
        "#3000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_3000\"),\n",
        "                               vocab_size=3000)\n",
        "\n",
        "#4000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_4000\"),\n",
        "                               vocab_size=4000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYweZ4Cmfet1"
      },
      "source": [
        "We prepare the various datasets with 1000 wods vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AnAUlyuKW9V",
        "outputId": "81ce58df-dda8-4d3e-ee24-b0b625da2348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19472 19472\n",
            "2615 2615\n",
            "5879 5879\n"
          ]
        }
      ],
      "source": [
        "#Loading datasets\n",
        "train_src = read_file(os.path.join(DIRECTORY,'data/train.src'))\n",
        "train_trg = read_file(os.path.join(DIRECTORY,'data/train.trg'))\n",
        "dev_src = read_file(os.path.join(DIRECTORY,'data/dev.src'))\n",
        "dev_trg = read_file(os.path.join(DIRECTORY,'data/dev.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "\n",
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_1000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp1000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp1000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp1000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp1000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp1000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp1000.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpu6eFkJKW9W"
      },
      "source": [
        "2000 words vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q4vTqi6fiCl",
        "outputId": "de1c56fe-f9d4-4d1e-b077-65bda1f2cb97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19472 19472\n",
            "2615 2615\n",
            "5879 5879\n"
          ]
        }
      ],
      "source": [
        "#Loading datasets\n",
        "train_src = read_file(os.path.join(DIRECTORY,'data/train.src'))\n",
        "train_trg = read_file(os.path.join(DIRECTORY,'data/train.trg'))\n",
        "dev_src = read_file(os.path.join(DIRECTORY,'data/dev.src'))\n",
        "dev_trg = read_file(os.path.join(DIRECTORY,'data/dev.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "\n",
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_2000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp2000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp2000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp2000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp2000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp2000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp2000.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtjAPohKW9W"
      },
      "source": [
        "3000 words vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ML_XId-KW9W",
        "outputId": "83832404-67d3-40c5-9347-4ffd63b5c40a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19472 19472\n",
            "2615 2615\n",
            "5879 5879\n"
          ]
        }
      ],
      "source": [
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_3000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp3000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp3000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp3000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp3000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp3000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp3000.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QThTgqDKW9W"
      },
      "source": [
        "4000 words vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XupMx_7SKW9W",
        "outputId": "19e6e5d9-2a1f-4c1e-fec6-ad00cbdb5cc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19472 19472\n",
            "2615 2615\n",
            "5879 5879\n"
          ]
        }
      ],
      "source": [
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_4000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp4000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp4000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp4000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp4000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp4000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp4000.trg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngPU1B3wg4hG",
        "outputId": "b69f9dcf-d5de-4005-8221-8cf66d6ad9a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-20 20:34:03 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='trg', target_lang='src', trainpref='FreEMnorm/data/train.sp1000', validpref='FreEMnorm/data/dev.sp1000', testpref='FreEMnorm/data/test.sp1000', align_suffix=None, destdir='FreEMnorm/data/data_norm_bin_1000/', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=True, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-06-20 20:34:06 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 952 types\n",
            "2024-06-20 20:34:12 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/train.sp1000.trg: 19472 sents, 518929 tokens, 0.0% replaced (by <unk>)\n",
            "2024-06-20 20:34:12 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 952 types\n",
            "2024-06-20 20:34:12 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/dev.sp1000.trg: 2615 sents, 78887 tokens, 0.057% replaced (by <unk>)\n",
            "2024-06-20 20:34:12 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 952 types\n",
            "2024-06-20 20:34:14 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/test.sp1000.trg: 5879 sents, 165345 tokens, 0.0363% replaced (by <unk>)\n",
            "2024-06-20 20:34:14 | INFO | fairseq_cli.preprocess | [src] Dictionary: 952 types\n",
            "2024-06-20 20:34:19 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/train.sp1000.src: 19472 sents, 522037 tokens, 0.0% replaced (by <unk>)\n",
            "2024-06-20 20:34:19 | INFO | fairseq_cli.preprocess | [src] Dictionary: 952 types\n",
            "2024-06-20 20:34:20 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/dev.sp1000.src: 2615 sents, 79373 tokens, 0.0605% replaced (by <unk>)\n",
            "2024-06-20 20:34:20 | INFO | fairseq_cli.preprocess | [src] Dictionary: 952 types\n",
            "2024-06-20 20:34:22 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/test.sp1000.src: 5879 sents, 166664 tokens, 0.0354% replaced (by <unk>)\n",
            "2024-06-20 20:34:22 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to FreEMnorm/data/data_norm_bin_1000/\n",
            "2024-06-20 20:34:36 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='trg', target_lang='src', trainpref='FreEMnorm/data/train.sp2000', validpref='FreEMnorm/data/dev.sp2000', testpref='FreEMnorm/data/test.sp2000', align_suffix=None, destdir='FreEMnorm/data/data_norm_bin_2000/', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=True, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-06-20 20:34:39 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 1840 types\n",
            "2024-06-20 20:34:44 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/train.sp2000.trg: 19472 sents, 450667 tokens, 0.0% replaced (by <unk>)\n",
            "2024-06-20 20:34:44 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 1840 types\n",
            "2024-06-20 20:34:44 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/dev.sp2000.trg: 2615 sents, 68418 tokens, 0.0672% replaced (by <unk>)\n",
            "2024-06-20 20:34:44 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 1840 types\n",
            "2024-06-20 20:34:46 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/test.sp2000.trg: 5879 sents, 142305 tokens, 0.0689% replaced (by <unk>)\n",
            "2024-06-20 20:34:46 | INFO | fairseq_cli.preprocess | [src] Dictionary: 1840 types\n",
            "2024-06-20 20:34:51 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/train.sp2000.src: 19472 sents, 459665 tokens, 0.0% replaced (by <unk>)\n",
            "2024-06-20 20:34:51 | INFO | fairseq_cli.preprocess | [src] Dictionary: 1840 types\n",
            "2024-06-20 20:34:52 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/dev.sp2000.src: 2615 sents, 69545 tokens, 0.0719% replaced (by <unk>)\n",
            "2024-06-20 20:34:52 | INFO | fairseq_cli.preprocess | [src] Dictionary: 1840 types\n",
            "2024-06-20 20:34:53 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/test.sp2000.src: 5879 sents, 145183 tokens, 0.0661% replaced (by <unk>)\n",
            "2024-06-20 20:34:53 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to FreEMnorm/data/data_norm_bin_2000/\n",
            "2024-06-20 20:35:02 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='trg', target_lang='src', trainpref='FreEMnorm/data/train.sp3000', validpref='FreEMnorm/data/dev.sp3000', testpref='FreEMnorm/data/test.sp3000', align_suffix=None, destdir='FreEMnorm/data/data_norm_bin_3000/', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=True, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-06-20 20:35:05 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 2624 types\n",
            "2024-06-20 20:35:10 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/train.sp3000.trg: 19472 sents, 420970 tokens, 0.0% replaced (by <unk>)\n",
            "2024-06-20 20:35:10 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 2624 types\n",
            "2024-06-20 20:35:11 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/dev.sp3000.trg: 2615 sents, 63751 tokens, 0.162% replaced (by <unk>)\n",
            "2024-06-20 20:35:11 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 2624 types\n",
            "2024-06-20 20:35:12 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/test.sp3000.trg: 5879 sents, 132609 tokens, 0.128% replaced (by <unk>)\n",
            "2024-06-20 20:35:12 | INFO | fairseq_cli.preprocess | [src] Dictionary: 2624 types\n",
            "2024-06-20 20:35:17 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/train.sp3000.src: 19472 sents, 428918 tokens, 0.0% replaced (by <unk>)\n",
            "2024-06-20 20:35:17 | INFO | fairseq_cli.preprocess | [src] Dictionary: 2624 types\n",
            "2024-06-20 20:35:17 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/dev.sp3000.src: 2615 sents, 64769 tokens, 0.151% replaced (by <unk>)\n",
            "2024-06-20 20:35:17 | INFO | fairseq_cli.preprocess | [src] Dictionary: 2624 types\n",
            "2024-06-20 20:35:19 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/test.sp3000.src: 5879 sents, 134937 tokens, 0.138% replaced (by <unk>)\n",
            "2024-06-20 20:35:19 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to FreEMnorm/data/data_norm_bin_3000/\n",
            "2024-06-20 20:35:28 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='trg', target_lang='src', trainpref='FreEMnorm/data/train.sp4000', validpref='FreEMnorm/data/dev.sp4000', testpref='FreEMnorm/data/test.sp4000', align_suffix=None, destdir='FreEMnorm/data/data_norm_bin_4000/', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=True, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-06-20 20:35:31 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 3256 types\n",
            "2024-06-20 20:35:36 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/train.sp4000.trg: 19472 sents, 403970 tokens, 0.0% replaced (by <unk>)\n",
            "2024-06-20 20:35:36 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 3256 types\n",
            "2024-06-20 20:35:36 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/dev.sp4000.trg: 2615 sents, 61080 tokens, 0.257% replaced (by <unk>)\n",
            "2024-06-20 20:35:36 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 3256 types\n",
            "2024-06-20 20:35:38 | INFO | fairseq_cli.preprocess | [trg] FreEMnorm/data/test.sp4000.trg: 5879 sents, 127081 tokens, 0.178% replaced (by <unk>)\n",
            "2024-06-20 20:35:38 | INFO | fairseq_cli.preprocess | [src] Dictionary: 3256 types\n",
            "2024-06-20 20:35:42 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/train.sp4000.src: 19472 sents, 412317 tokens, 0.0% replaced (by <unk>)\n",
            "2024-06-20 20:35:42 | INFO | fairseq_cli.preprocess | [src] Dictionary: 3256 types\n",
            "2024-06-20 20:35:43 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/dev.sp4000.src: 2615 sents, 62138 tokens, 0.245% replaced (by <unk>)\n",
            "2024-06-20 20:35:43 | INFO | fairseq_cli.preprocess | [src] Dictionary: 3256 types\n",
            "2024-06-20 20:35:44 | INFO | fairseq_cli.preprocess | [src] FreEMnorm/data/test.sp4000.src: 5879 sents, 129971 tokens, 0.162% replaced (by <unk>)\n",
            "2024-06-20 20:35:44 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to FreEMnorm/data/data_norm_bin_4000/\n"
          ]
        }
      ],
      "source": [
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_1000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp1000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp1000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp1000 \\\n",
        "                    --joined-dictionary\n",
        "\n",
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_2000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp2000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp2000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp2000 \\\n",
        "                    --joined-dictionary\n",
        "\n",
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_3000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp3000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp3000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp3000 \\\n",
        "                    --joined-dictionary\n",
        "\n",
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_4000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp4000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp4000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp4000 \\\n",
        "                    --joined-dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yqxh3vSnZSq"
      },
      "source": [
        "### II.b Training\n",
        "\n",
        "We now train a model with a vocab of 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5X-AMlzKW9X"
      },
      "outputs": [],
      "source": [
        "!pip uninstall numpy\n",
        "!pip install numpy==1.25.2\n",
        "import numpy\n",
        "numpy.version.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T583f25KW9X",
        "outputId": "819667a4-d8b2-47ca-a09f-bfecd34f31b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-20 20:38:42 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'FreEMnorm/models/lstm_dict1000_3l_embed384', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 12, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3000, batch_size_valid='64', max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='lstm', max_epoch=0, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='FreEMnorm/models/lstm_dict1000_3l_embed384', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=12, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, share_decoder_input_output_embed=False, share_all_embeddings=True, data='FreEMnorm/data/data_norm_bin_1000', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_layers=3, decoder_layers=3, encoder_embed_dim=384, decoder_embed_dim=384, decoder_out_embed_dim=384, encoder_hidden_size=768, encoder_bidirectional=True, decoder_hidden_size=768, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_freeze_embed=False, encoder_dropout_in=0.3, encoder_dropout_out=0.3, decoder_embed_path=None, decoder_freeze_embed=False, decoder_attention='1', decoder_dropout_in=0.3, decoder_dropout_out=0.3, adaptive_softmax_cutoff='10000,50000,200000', _name='lstm'), 'task': {'_name': 'translation', 'data': 'FreEMnorm/data/data_norm_bin_1000', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-06-20 20:38:42 | INFO | fairseq.tasks.translation | [trg] dictionary: 952 types\n",
            "2024-06-20 20:38:42 | INFO | fairseq.tasks.translation | [src] dictionary: 952 types\n",
            "2024-06-20 20:38:43 | INFO | fairseq_cli.train | LSTMModel(\n",
            "  (encoder): LSTMEncoder(\n",
            "    (dropout_in_module): FairseqDropout()\n",
            "    (dropout_out_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(952, 384, padding_idx=1)\n",
            "    (lstm): LSTM(384, 768, num_layers=3, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): LSTMDecoder(\n",
            "    (dropout_in_module): FairseqDropout()\n",
            "    (dropout_out_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(952, 384, padding_idx=1)\n",
            "    (encoder_hidden_proj): Linear(in_features=1536, out_features=768, bias=True)\n",
            "    (encoder_cell_proj): Linear(in_features=1536, out_features=768, bias=True)\n",
            "    (layers): ModuleList(\n",
            "      (0): LSTMCell(1152, 768)\n",
            "      (1-2): 2 x LSTMCell(768, 768)\n",
            "    )\n",
            "    (attention): AttentionLayer(\n",
            "      (input_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
            "      (output_proj): Linear(in_features=2304, out_features=768, bias=False)\n",
            "    )\n",
            "    (additional_fc): Linear(in_features=768, out_features=384, bias=True)\n",
            "  )\n",
            ")\n",
            "2024-06-20 20:38:43 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-06-20 20:38:43 | INFO | fairseq_cli.train | model: LSTMModel\n",
            "2024-06-20 20:38:43 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
            "2024-06-20 20:38:43 | INFO | fairseq_cli.train | num. shared model params: 56,750,976 (num. trained: 56,750,976)\n",
            "2024-06-20 20:38:43 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-06-20 20:38:43 | INFO | fairseq.data.data_utils | loaded 2,615 examples from: FreEMnorm/data/data_norm_bin_1000/valid.trg-src.trg\n",
            "2024-06-20 20:38:43 | INFO | fairseq.data.data_utils | loaded 2,615 examples from: FreEMnorm/data/data_norm_bin_1000/valid.trg-src.src\n",
            "2024-06-20 20:38:43 | INFO | fairseq.tasks.translation | FreEMnorm/data/data_norm_bin_1000 valid trg-src 2615 examples\n",
            "2024-06-20 20:38:44 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
            "2024-06-20 20:38:44 | INFO | fairseq.trainer | detected shared parameter: decoder.attention.input_proj.bias <- decoder.attention.output_proj.bias\n",
            "2024-06-20 20:38:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-06-20 20:38:44 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 11.901 GB ; name = Tesla P100-PCIE-12GB                    \n",
            "2024-06-20 20:38:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-06-20 20:38:44 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-06-20 20:38:44 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None\n",
            "2024-06-20 20:38:44 | INFO | fairseq.trainer | Preparing to load checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint_last.pt\n",
            "2024-06-20 20:38:44 | INFO | fairseq.trainer | No existing checkpoint found FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint_last.pt\n",
            "2024-06-20 20:38:44 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-06-20 20:38:44 | INFO | fairseq.data.data_utils | loaded 19,472 examples from: FreEMnorm/data/data_norm_bin_1000/train.trg-src.trg\n",
            "2024-06-20 20:38:44 | INFO | fairseq.data.data_utils | loaded 19,472 examples from: FreEMnorm/data/data_norm_bin_1000/train.trg-src.src\n",
            "2024-06-20 20:38:44 | INFO | fairseq.tasks.translation | FreEMnorm/data/data_norm_bin_1000 train trg-src 19472 examples\n",
            "2024-06-20 20:38:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 001:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:38:46 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-06-20 20:38:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/home/users/g/gabays/.local/lib/python3.10/site-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100%|▉| 210/211 [01:23<00:00,  4.00it/s, loss=8.644, ppl=399.96, wps=2024-06-20 20:40:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6%|▍      | 3/51 [00:00<00:01, 25.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14%|▉      | 7/51 [00:00<00:01, 30.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22%|█▎    | 11/51 [00:00<00:01, 30.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29%|█▊    | 15/51 [00:00<00:01, 29.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35%|██    | 18/51 [00:00<00:01, 28.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41%|██▍   | 21/51 [00:00<00:01, 27.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47%|██▊   | 24/51 [00:00<00:01, 26.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53%|███▏  | 27/51 [00:01<00:00, 24.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59%|███▌  | 30/51 [00:01<00:00, 23.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65%|███▉  | 33/51 [00:01<00:00, 19.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71%|████▏ | 36/51 [00:01<00:00, 15.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75%|████▍ | 38/51 [00:01<00:00, 13.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78%|████▋ | 40/51 [00:02<00:00, 12.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82%|████▉ | 42/51 [00:02<00:00, 11.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86%|█████▏| 44/51 [00:02<00:00, 10.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  90%|█████▍| 46/51 [00:02<00:00,  8.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.04it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:40:14 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.977 | ppl 251.89 | wps 20673 | wpb 1556.3 | bsz 51.3 | num_updates 211\n",
            "2024-06-20 20:40:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 211 updates\n",
            "2024-06-20 20:40:14 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint1.pt\n",
            "2024-06-20 20:40:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint1.pt\n",
            "2024-06-20 20:40:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint1.pt (epoch 1 @ 211 updates, score 7.977) (writing took 2.9554147629532963 seconds)\n",
            "2024-06-20 20:40:17 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-06-20 20:40:17 | INFO | train | epoch 001 | loss 9.09 | ppl 544.95 | wps 6053.3 | ups 2.45 | wpb 2474.1 | bsz 92.3 | num_updates 211 | lr 5.275e-05 | gnorm 2.045 | train_wall 83 | gb_free 10.3 | wall 93\n",
            "2024-06-20 20:40:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 002:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:40:17 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-06-20 20:40:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100%|▉| 210/211 [01:18<00:00,  2.33it/s, loss=7.708, ppl=209.12, wps=2024-06-20 20:41:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.17it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:41:40 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.1 | ppl 137.18 | wps 20678.4 | wpb 1556.3 | bsz 51.3 | num_updates 422 | best_loss 7.1\n",
            "2024-06-20 20:41:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 422 updates\n",
            "2024-06-20 20:41:40 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint2.pt\n",
            "2024-06-20 20:41:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint2.pt\n",
            "2024-06-20 20:41:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint2.pt (epoch 2 @ 422 updates, score 7.1) (writing took 2.9546338469954208 seconds)\n",
            "2024-06-20 20:41:43 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-06-20 20:41:43 | INFO | train | epoch 002 | loss 7.866 | ppl 233.22 | wps 6090.3 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 422 | lr 0.0001055 | gnorm 2.043 | train_wall 78 | gb_free 10.4 | wall 179\n",
            "2024-06-20 20:41:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 003:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:41:43 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-06-20 20:41:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100%|▉| 210/211 [01:18<00:00,  2.04it/s, loss=6.745, ppl=107.26, wps=2024-06-20 20:43:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.46it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.42it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.48it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.11it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.74it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.01it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.89it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.75it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.74it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.14it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:43:06 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.039 | ppl 65.76 | wps 20647.1 | wpb 1556.3 | bsz 51.3 | num_updates 633 | best_loss 6.039\n",
            "2024-06-20 20:43:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 633 updates\n",
            "2024-06-20 20:43:06 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint3.pt\n",
            "2024-06-20 20:43:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint3.pt\n",
            "2024-06-20 20:43:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint3.pt (epoch 3 @ 633 updates, score 6.039) (writing took 3.016710811993107 seconds)\n",
            "2024-06-20 20:43:09 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-06-20 20:43:09 | INFO | train | epoch 003 | loss 6.935 | ppl 122.37 | wps 6066.7 | ups 2.45 | wpb 2474.1 | bsz 92.3 | num_updates 633 | lr 0.00015825 | gnorm 2.455 | train_wall 78 | gb_free 10.4 | wall 265\n",
            "2024-06-20 20:43:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 004:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:43:09 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-06-20 20:43:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100%|▉| 210/211 [01:18<00:00,  1.62it/s, loss=6.035, ppl=65.58, wps=62024-06-20 20:44:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 38.77it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.01it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.24it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.25it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 26.89it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 23.90it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.66it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.85it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.73it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.74it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.15it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:44:32 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.567 | ppl 47.4 | wps 20598.3 | wpb 1556.3 | bsz 51.3 | num_updates 844 | best_loss 5.567\n",
            "2024-06-20 20:44:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 844 updates\n",
            "2024-06-20 20:44:32 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint4.pt\n",
            "2024-06-20 20:44:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint4.pt\n",
            "2024-06-20 20:44:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint4.pt (epoch 4 @ 844 updates, score 5.567) (writing took 3.019538890104741 seconds)\n",
            "2024-06-20 20:44:35 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-06-20 20:44:35 | INFO | train | epoch 004 | loss 6.036 | ppl 65.61 | wps 6067.3 | ups 2.45 | wpb 2474.1 | bsz 92.3 | num_updates 844 | lr 0.000211 | gnorm 2.796 | train_wall 78 | gb_free 10.3 | wall 351\n",
            "2024-06-20 20:44:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 005:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:44:35 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-06-20 20:44:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100%|▉| 210/211 [01:18<00:00,  1.96it/s, loss=5.12, ppl=34.77, wps=682024-06-20 20:45:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.81it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.51it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.57it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.28it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.09it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.74it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.00it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.42it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.69it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.72it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.76it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.16it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:45:58 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 3.99 | ppl 15.89 | wps 20629.2 | wpb 1556.3 | bsz 51.3 | num_updates 1055 | best_loss 3.99\n",
            "2024-06-20 20:45:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1055 updates\n",
            "2024-06-20 20:45:58 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint5.pt\n",
            "2024-06-20 20:45:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint5.pt\n",
            "2024-06-20 20:46:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint5.pt (epoch 5 @ 1055 updates, score 3.99) (writing took 3.1742849140428007 seconds)\n",
            "2024-06-20 20:46:01 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-06-20 20:46:01 | INFO | train | epoch 005 | loss 5.107 | ppl 34.47 | wps 6059.7 | ups 2.45 | wpb 2474.1 | bsz 92.3 | num_updates 1055 | lr 0.00026375 | gnorm 2.898 | train_wall 78 | gb_free 10.5 | wall 437\n",
            "2024-06-20 20:46:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 006:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:46:01 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-06-20 20:46:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006: 100%|▉| 210/211 [01:18<00:00,  2.45it/s, loss=3.87, ppl=14.62, wps=702024-06-20 20:47:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  10%|▋      | 5/51 [00:00<00:01, 38.60it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  18%|█▏     | 9/51 [00:00<00:01, 34.68it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25%|█▌    | 13/51 [00:00<00:01, 32.49it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33%|██    | 17/51 [00:00<00:01, 29.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  41%|██▍   | 21/51 [00:00<00:01, 28.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  47%|██▊   | 24/51 [00:00<00:01, 26.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  53%|███▏  | 27/51 [00:00<00:00, 25.48it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  59%|███▌  | 30/51 [00:01<00:00, 23.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  65%|███▉  | 33/51 [00:01<00:00, 20.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  71%|████▏ | 36/51 [00:01<00:00, 16.12it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75%|████▍ | 38/51 [00:01<00:00, 13.80it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  78%|████▋ | 40/51 [00:02<00:00, 12.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82%|████▉ | 42/51 [00:02<00:00, 11.42it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  86%|█████▏| 44/51 [00:02<00:00, 10.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  90%|█████▍| 46/51 [00:02<00:00,  8.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.40it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.70it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.68it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.14it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.06it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:47:24 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 2.644 | ppl 6.25 | wps 20720.6 | wpb 1556.3 | bsz 51.3 | num_updates 1266 | best_loss 2.644\n",
            "2024-06-20 20:47:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1266 updates\n",
            "2024-06-20 20:47:24 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint6.pt\n",
            "2024-06-20 20:47:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint6.pt\n",
            "2024-06-20 20:47:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint6.pt (epoch 6 @ 1266 updates, score 2.644) (writing took 3.181391894002445 seconds)\n",
            "2024-06-20 20:47:27 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-06-20 20:47:27 | INFO | train | epoch 006 | loss 3.901 | ppl 14.93 | wps 6069.7 | ups 2.45 | wpb 2474.1 | bsz 92.3 | num_updates 1266 | lr 0.0003165 | gnorm 3.392 | train_wall 78 | gb_free 10.5 | wall 523\n",
            "2024-06-20 20:47:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 007:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:47:27 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2024-06-20 20:47:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007: 100%|▉| 210/211 [01:18<00:00,  3.40it/s, loss=2.744, ppl=6.7, wps=6562024-06-20 20:48:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.53it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.56it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.17it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.84it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.12it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.53it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.26it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.71it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.99it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.84it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.71it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.59it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.73it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.77it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.23it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.15it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:48:50 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 1.456 | ppl 2.74 | wps 20652 | wpb 1556.3 | bsz 51.3 | num_updates 1477 | best_loss 1.456\n",
            "2024-06-20 20:48:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1477 updates\n",
            "2024-06-20 20:48:50 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint7.pt\n",
            "2024-06-20 20:48:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint7.pt\n",
            "2024-06-20 20:48:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint7.pt (epoch 7 @ 1477 updates, score 1.456) (writing took 3.231057911994867 seconds)\n",
            "2024-06-20 20:48:53 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2024-06-20 20:48:53 | INFO | train | epoch 007 | loss 2.651 | ppl 6.28 | wps 6057.9 | ups 2.45 | wpb 2474.1 | bsz 92.3 | num_updates 1477 | lr 0.00036925 | gnorm 2.404 | train_wall 78 | gb_free 10.4 | wall 609\n",
            "2024-06-20 20:48:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 008:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:48:53 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2024-06-20 20:48:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008: 100%|▉| 210/211 [01:18<00:00,  1.46it/s, loss=1.892, ppl=3.71, wps=672024-06-20 20:50:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.22it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.14it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.34it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.44it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.13it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.06it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.69it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 23.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.11it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.62it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.89it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.77it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.68it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.72it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.77it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.24it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.17it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:50:16 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 1.315 | ppl 2.49 | wps 20588 | wpb 1556.3 | bsz 51.3 | num_updates 1688 | best_loss 1.315\n",
            "2024-06-20 20:50:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1688 updates\n",
            "2024-06-20 20:50:16 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint8.pt\n",
            "2024-06-20 20:50:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint8.pt\n",
            "2024-06-20 20:50:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint8.pt (epoch 8 @ 1688 updates, score 1.315) (writing took 2.9437316119438037 seconds)\n",
            "2024-06-20 20:50:19 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2024-06-20 20:50:19 | INFO | train | epoch 008 | loss 1.79 | ppl 3.46 | wps 6079.7 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 1688 | lr 0.000422 | gnorm 1.947 | train_wall 78 | gb_free 10.5 | wall 695\n",
            "2024-06-20 20:50:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 009:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:50:19 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2024-06-20 20:50:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009: 100%|▉| 210/211 [01:18<00:00,  3.28it/s, loss=1.562, ppl=2.95, wps=652024-06-20 20:51:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.46it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.13it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.35it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.45it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.23it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.09it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.78it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.08it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.53it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.27it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.74it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.01it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.89it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.77it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.64it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.38it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.76it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.80it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.25it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.17it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:51:42 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 0.948 | ppl 1.93 | wps 20692.1 | wpb 1556.3 | bsz 51.3 | num_updates 1899 | best_loss 0.948\n",
            "2024-06-20 20:51:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1899 updates\n",
            "2024-06-20 20:51:42 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint9.pt\n",
            "2024-06-20 20:51:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint9.pt\n",
            "2024-06-20 20:51:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint9.pt (epoch 9 @ 1899 updates, score 0.948) (writing took 3.494945760932751 seconds)\n",
            "2024-06-20 20:51:46 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2024-06-20 20:51:46 | INFO | train | epoch 009 | loss 1.418 | ppl 2.67 | wps 6042.7 | ups 2.44 | wpb 2474.1 | bsz 92.3 | num_updates 1899 | lr 0.00047475 | gnorm 1.622 | train_wall 78 | gb_free 10.4 | wall 782\n",
            "2024-06-20 20:51:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 010:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:51:46 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2024-06-20 20:51:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010: 100%|▉| 210/211 [01:18<00:00,  2.50it/s, loss=1.18, ppl=2.27, wps=6552024-06-20 20:53:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 38.87it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 34.70it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 31.98it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 29.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 27.82it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 26.81it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.55it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 23.89it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.40it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.22it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.71it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.99it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.87it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.74it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.61it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.35it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.73it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.22it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.14it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:53:08 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.797 | ppl 1.74 | wps 20582.7 | wpb 1556.3 | bsz 51.3 | num_updates 2110 | best_loss 0.797\n",
            "2024-06-20 20:53:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 2110 updates\n",
            "2024-06-20 20:53:08 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint10.pt\n",
            "2024-06-20 20:53:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint10.pt\n",
            "2024-06-20 20:53:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint10.pt (epoch 10 @ 2110 updates, score 0.797) (writing took 3.0335571239702404 seconds)\n",
            "2024-06-20 20:53:11 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2024-06-20 20:53:11 | INFO | train | epoch 010 | loss 1.223 | ppl 2.33 | wps 6075.1 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 2110 | lr 0.0005275 | gnorm 1.588 | train_wall 78 | gb_free 10.4 | wall 868\n",
            "2024-06-20 20:53:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 011:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:53:11 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2024-06-20 20:53:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011: 100%|▉| 210/211 [01:18<00:00,  2.62it/s, loss=1.064, ppl=2.09, wps=642024-06-20 20:54:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.24it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.14it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.46it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.29it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.15it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.84it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.14it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.55it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.28it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.75it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.01it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.88it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.76it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.37it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.75it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.79it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.23it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.15it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:54:34 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.692 | ppl 1.62 | wps 20665.4 | wpb 1556.3 | bsz 51.3 | num_updates 2321 | best_loss 0.692\n",
            "2024-06-20 20:54:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2321 updates\n",
            "2024-06-20 20:54:34 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint11.pt\n",
            "2024-06-20 20:54:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint11.pt\n",
            "2024-06-20 20:54:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint11.pt (epoch 11 @ 2321 updates, score 0.692) (writing took 3.0719318769406527 seconds)\n",
            "2024-06-20 20:54:37 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2024-06-20 20:54:37 | INFO | train | epoch 011 | loss 1.056 | ppl 2.08 | wps 6082.7 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 2321 | lr 0.00058025 | gnorm 1.356 | train_wall 78 | gb_free 10.4 | wall 953\n",
            "2024-06-20 20:54:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 012:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:54:37 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2024-06-20 20:54:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012: 100%|▉| 210/211 [01:18<00:00,  2.54it/s, loss=1.238, ppl=2.36, wps=672024-06-20 20:55:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  10%|▋      | 5/51 [00:00<00:01, 38.67it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  18%|█▏     | 9/51 [00:00<00:01, 34.59it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  25%|█▌    | 13/51 [00:00<00:01, 32.22it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33%|██    | 17/51 [00:00<00:01, 29.74it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.24it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 26.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.65it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 23.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.40it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.16it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.67it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.86it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.74it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.36it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.73it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.22it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.14it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:56:00 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.678 | ppl 1.6 | wps 20652.5 | wpb 1556.3 | bsz 51.3 | num_updates 2532 | best_loss 0.678\n",
            "2024-06-20 20:56:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2532 updates\n",
            "2024-06-20 20:56:00 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint12.pt\n",
            "2024-06-20 20:56:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint12.pt\n",
            "2024-06-20 20:56:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint12.pt (epoch 12 @ 2532 updates, score 0.678) (writing took 2.9418123239884153 seconds)\n",
            "2024-06-20 20:56:03 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2024-06-20 20:56:03 | INFO | train | epoch 012 | loss 1.043 | ppl 2.06 | wps 6094.6 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 2532 | lr 0.000633 | gnorm 1.467 | train_wall 78 | gb_free 10.4 | wall 1039\n",
            "2024-06-20 20:56:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 013:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:56:03 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2024-06-20 20:56:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013: 100%|▉| 210/211 [01:18<00:00,  3.58it/s, loss=0.944, ppl=1.92, wps=632024-06-20 20:57:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.80it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.51it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.58it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.56it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.34it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.18it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.84it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.12it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.52it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.26it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.74it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.01it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.90it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.77it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.36it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.75it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.79it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.23it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.14it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:57:26 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 0.644 | ppl 1.56 | wps 20668.6 | wpb 1556.3 | bsz 51.3 | num_updates 2743 | best_loss 0.644\n",
            "2024-06-20 20:57:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2743 updates\n",
            "2024-06-20 20:57:26 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint13.pt\n",
            "2024-06-20 20:57:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint13.pt\n",
            "2024-06-20 20:57:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint13.pt (epoch 13 @ 2743 updates, score 0.644) (writing took 3.2701252090046182 seconds)\n",
            "2024-06-20 20:57:29 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2024-06-20 20:57:29 | INFO | train | epoch 013 | loss 0.868 | ppl 1.82 | wps 6060.9 | ups 2.45 | wpb 2474.1 | bsz 92.3 | num_updates 2743 | lr 0.00068575 | gnorm 1.117 | train_wall 78 | gb_free 10.4 | wall 1125\n",
            "2024-06-20 20:57:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 014:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:57:29 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2024-06-20 20:57:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014: 100%|▉| 210/211 [01:18<00:00,  2.76it/s, loss=0.739, ppl=1.67, wps=722024-06-20 20:58:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.94it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.48it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.54it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.57it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.32it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.17it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.83it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.10it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.52it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.27it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.75it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.02it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.89it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.76it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.36it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.75it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.79it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.28it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.17it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 20:58:52 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 0.663 | ppl 1.58 | wps 20707.7 | wpb 1556.3 | bsz 51.3 | num_updates 2954 | best_loss 0.644\n",
            "2024-06-20 20:58:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2954 updates\n",
            "2024-06-20 20:58:52 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint14.pt\n",
            "2024-06-20 20:58:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint14.pt\n",
            "2024-06-20 20:58:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint14.pt (epoch 14 @ 2954 updates, score 0.663) (writing took 1.8727757900487632 seconds)\n",
            "2024-06-20 20:58:54 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2024-06-20 20:58:54 | INFO | train | epoch 014 | loss 0.782 | ppl 1.72 | wps 6161.9 | ups 2.49 | wpb 2474.1 | bsz 92.3 | num_updates 2954 | lr 0.0007385 | gnorm 0.931 | train_wall 78 | gb_free 10.6 | wall 1210\n",
            "2024-06-20 20:58:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 015:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 20:58:54 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2024-06-20 20:58:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015: 100%|▉| 210/211 [01:18<00:00,  3.10it/s, loss=0.795, ppl=1.73, wps=632024-06-20 21:00:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.82it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.58it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.68it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.67it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.39it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.22it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.90it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.17it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.58it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.34it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.78it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.03it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.90it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.78it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.65it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.39it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.77it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.80it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.24it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.16it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:00:17 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 0.642 | ppl 1.56 | wps 20724.9 | wpb 1556.3 | bsz 51.3 | num_updates 3165 | best_loss 0.642\n",
            "2024-06-20 21:00:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 3165 updates\n",
            "2024-06-20 21:00:17 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint15.pt\n",
            "2024-06-20 21:00:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint15.pt\n",
            "2024-06-20 21:00:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint15.pt (epoch 15 @ 3165 updates, score 0.642) (writing took 2.9622461179969832 seconds)\n",
            "2024-06-20 21:00:20 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2024-06-20 21:00:20 | INFO | train | epoch 015 | loss 0.828 | ppl 1.78 | wps 6087 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 3165 | lr 0.00079125 | gnorm 1.118 | train_wall 78 | gb_free 10.5 | wall 1296\n",
            "2024-06-20 21:00:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 016:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:00:20 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2024-06-20 21:00:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016: 100%|▉| 210/211 [01:18<00:00,  2.81it/s, loss=0.688, ppl=1.61, wps=632024-06-20 21:01:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  10%|▋      | 5/51 [00:00<00:01, 38.76it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  18%|█▏     | 9/51 [00:00<00:01, 34.66it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  25%|█▌    | 13/51 [00:00<00:01, 32.47it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  33%|██    | 17/51 [00:00<00:01, 29.98it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  41%|██▍   | 21/51 [00:00<00:01, 28.22it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  47%|██▊   | 24/51 [00:00<00:01, 26.91it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  53%|███▏  | 27/51 [00:00<00:00, 25.42it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  59%|███▌  | 30/51 [00:01<00:00, 23.49it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  65%|███▉  | 33/51 [00:01<00:00, 20.18it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  71%|████▏ | 36/51 [00:01<00:00, 16.09it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  75%|████▍ | 38/51 [00:01<00:00, 13.75it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  78%|████▋ | 40/51 [00:02<00:00, 12.50it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  82%|████▉ | 42/51 [00:02<00:00, 11.37it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  86%|█████▏| 44/51 [00:02<00:00, 10.28it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  90%|█████▍| 46/51 [00:02<00:00,  8.91it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.38it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.68it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.65it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.12it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.05it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:01:42 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 0.524 | ppl 1.44 | wps 20661.9 | wpb 1556.3 | bsz 51.3 | num_updates 3376 | best_loss 0.524\n",
            "2024-06-20 21:01:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3376 updates\n",
            "2024-06-20 21:01:42 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint16.pt\n",
            "2024-06-20 21:01:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint16.pt\n",
            "2024-06-20 21:01:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint16.pt (epoch 16 @ 3376 updates, score 0.524) (writing took 3.0754414359107614 seconds)\n",
            "2024-06-20 21:01:45 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2024-06-20 21:01:45 | INFO | train | epoch 016 | loss 0.697 | ppl 1.62 | wps 6082.6 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 3376 | lr 0.000844 | gnorm 0.742 | train_wall 78 | gb_free 10.4 | wall 1382\n",
            "2024-06-20 21:01:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 017:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:01:45 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2024-06-20 21:01:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017: 100%|▉| 210/211 [01:21<00:00,  3.06it/s, loss=0.83, ppl=1.78, wps=6002024-06-20 21:03:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.25it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.15it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.43it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.51it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.32it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.17it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.82it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.09it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.51it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.24it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.71it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.97it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.84it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.73it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.58it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.34it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.73it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.24it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.16it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:03:10 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 0.483 | ppl 1.4 | wps 20651 | wpb 1556.3 | bsz 51.3 | num_updates 3587 | best_loss 0.483\n",
            "2024-06-20 21:03:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3587 updates\n",
            "2024-06-20 21:03:10 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint17.pt\n",
            "2024-06-20 21:03:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint17.pt\n",
            "2024-06-20 21:03:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint17.pt (epoch 17 @ 3587 updates, score 0.483) (writing took 3.5303279599174857 seconds)\n",
            "2024-06-20 21:03:14 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2024-06-20 21:03:14 | INFO | train | epoch 017 | loss 0.7 | ppl 1.62 | wps 5888 | ups 2.38 | wpb 2474.1 | bsz 92.3 | num_updates 3587 | lr 0.00089675 | gnorm 0.796 | train_wall 78 | gb_free 10.4 | wall 1470\n",
            "2024-06-20 21:03:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 018:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:03:14 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2024-06-20 21:03:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018: 100%|▉| 210/211 [01:20<00:00,  3.26it/s, loss=0.677, ppl=1.6, wps=5892024-06-20 21:04:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.32it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.15it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.34it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.39it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.15it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.00it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.68it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.00it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.45it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.21it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.68it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.95it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.82it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.70it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.56it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.32it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.71it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.76it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.22it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.16it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:04:38 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 0.467 | ppl 1.38 | wps 20604.9 | wpb 1556.3 | bsz 51.3 | num_updates 3798 | best_loss 0.467\n",
            "2024-06-20 21:04:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3798 updates\n",
            "2024-06-20 21:04:38 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint18.pt\n",
            "2024-06-20 21:04:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint18.pt\n",
            "2024-06-20 21:04:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint18.pt (epoch 18 @ 3798 updates, score 0.467) (writing took 3.011994734988548 seconds)\n",
            "2024-06-20 21:04:41 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2024-06-20 21:04:41 | INFO | train | epoch 018 | loss 0.631 | ppl 1.55 | wps 5982.9 | ups 2.42 | wpb 2474.1 | bsz 92.3 | num_updates 3798 | lr 0.0009495 | gnorm 0.71 | train_wall 78 | gb_free 10.3 | wall 1558\n",
            "2024-06-20 21:04:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 019:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:04:41 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2024-06-20 21:04:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019: 100%|▉| 210/211 [01:18<00:00,  3.27it/s, loss=0.706, ppl=1.63, wps=672024-06-20 21:06:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.85it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.60it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.63it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.62it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.40it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.26it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.91it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.18it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.57it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.29it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.76it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.02it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.90it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.76it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.63it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.37it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.75it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.79it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.27it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.17it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:06:04 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 0.484 | ppl 1.4 | wps 20723.4 | wpb 1556.3 | bsz 51.3 | num_updates 4009 | best_loss 0.467\n",
            "2024-06-20 21:06:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 4009 updates\n",
            "2024-06-20 21:06:04 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint19.pt\n",
            "2024-06-20 21:06:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint19.pt\n",
            "2024-06-20 21:06:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint19.pt (epoch 19 @ 4009 updates, score 0.484) (writing took 1.9283184049418196 seconds)\n",
            "2024-06-20 21:06:06 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2024-06-20 21:06:06 | INFO | train | epoch 019 | loss 0.653 | ppl 1.57 | wps 6160.9 | ups 2.49 | wpb 2474.1 | bsz 92.3 | num_updates 4009 | lr 0.000998877 | gnorm 0.818 | train_wall 78 | gb_free 10.3 | wall 1642\n",
            "2024-06-20 21:06:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 020:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:06:06 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2024-06-20 21:06:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020: 100%|▉| 210/211 [01:17<00:00,  1.82it/s, loss=0.729, ppl=1.66, wps=732024-06-20 21:07:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  10%|▋      | 5/51 [00:00<00:01, 38.74it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  18%|█▏     | 9/51 [00:00<00:01, 34.55it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  25%|█▌    | 13/51 [00:00<00:01, 32.42it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  33%|██    | 17/51 [00:00<00:01, 29.94it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  41%|██▍   | 21/51 [00:00<00:01, 28.19it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  47%|██▊   | 24/51 [00:00<00:01, 26.93it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  53%|███▏  | 27/51 [00:00<00:00, 25.52it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  59%|███▌  | 30/51 [00:01<00:00, 23.56it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  65%|███▉  | 33/51 [00:01<00:00, 20.23it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  71%|████▏ | 36/51 [00:01<00:00, 16.12it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  75%|████▍ | 38/51 [00:01<00:00, 13.78it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  78%|████▋ | 40/51 [00:02<00:00, 12.52it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  82%|████▉ | 42/51 [00:02<00:00, 11.40it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  86%|█████▏| 44/51 [00:02<00:00, 10.28it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  90%|█████▍| 46/51 [00:02<00:00,  8.89it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.37it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.68it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.66it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.10it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.03it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:07:29 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 0.459 | ppl 1.37 | wps 20656.2 | wpb 1556.3 | bsz 51.3 | num_updates 4220 | best_loss 0.459\n",
            "2024-06-20 21:07:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 4220 updates\n",
            "2024-06-20 21:07:29 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint20.pt\n",
            "2024-06-20 21:07:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint20.pt\n",
            "2024-06-20 21:07:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint20.pt (epoch 20 @ 4220 updates, score 0.459) (writing took 3.0385204600170255 seconds)\n",
            "2024-06-20 21:07:32 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2024-06-20 21:07:32 | INFO | train | epoch 020 | loss 0.634 | ppl 1.55 | wps 6095.4 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 4220 | lr 0.000973585 | gnorm 0.853 | train_wall 78 | gb_free 10.6 | wall 1728\n",
            "2024-06-20 21:07:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 021:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:07:32 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2024-06-20 21:07:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021: 100%|▉| 210/211 [01:18<00:00,  2.88it/s, loss=0.496, ppl=1.41, wps=652024-06-20 21:08:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  10%|▋      | 5/51 [00:00<00:01, 38.76it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  18%|█▏     | 9/51 [00:00<00:01, 34.67it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  25%|█▌    | 13/51 [00:00<00:01, 32.54it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  33%|██    | 17/51 [00:00<00:01, 30.06it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  41%|██▍   | 21/51 [00:00<00:01, 28.24it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  47%|██▊   | 24/51 [00:00<00:01, 26.96it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  53%|███▏  | 27/51 [00:00<00:00, 25.56it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  59%|███▌  | 30/51 [00:01<00:00, 23.59it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  65%|███▉  | 33/51 [00:01<00:00, 20.23it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  71%|████▏ | 36/51 [00:01<00:00, 16.12it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  75%|████▍ | 38/51 [00:01<00:00, 13.78it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  78%|████▋ | 40/51 [00:02<00:00, 12.52it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  82%|████▉ | 42/51 [00:02<00:00, 11.39it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  86%|█████▏| 44/51 [00:02<00:00, 10.27it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  90%|█████▍| 46/51 [00:02<00:00,  8.90it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.37it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.67it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.65it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.13it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.06it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:08:54 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 0.431 | ppl 1.35 | wps 20685.3 | wpb 1556.3 | bsz 51.3 | num_updates 4431 | best_loss 0.431\n",
            "2024-06-20 21:08:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4431 updates\n",
            "2024-06-20 21:08:54 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint21.pt\n",
            "2024-06-20 21:08:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint21.pt\n",
            "2024-06-20 21:08:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint21.pt (epoch 21 @ 4431 updates, score 0.431) (writing took 3.033057563006878 seconds)\n",
            "2024-06-20 21:08:57 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2024-06-20 21:08:57 | INFO | train | epoch 021 | loss 0.502 | ppl 1.42 | wps 6089.8 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 4431 | lr 0.000950121 | gnorm 0.482 | train_wall 78 | gb_free 10.4 | wall 1814\n",
            "2024-06-20 21:08:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 022:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:08:57 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2024-06-20 21:08:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022: 100%|▉| 210/211 [01:18<00:00,  2.15it/s, loss=0.457, ppl=1.37, wps=692024-06-20 21:10:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.88it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.46it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.50it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.55it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.32it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.14it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.77it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.02it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.42it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.20it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.71it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.00it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.88it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.75it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.37it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.75it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.79it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.23it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.14it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:10:20 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 0.43 | ppl 1.35 | wps 20652.1 | wpb 1556.3 | bsz 51.3 | num_updates 4642 | best_loss 0.43\n",
            "2024-06-20 21:10:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4642 updates\n",
            "2024-06-20 21:10:20 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint22.pt\n",
            "2024-06-20 21:10:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint22.pt\n",
            "2024-06-20 21:10:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint22.pt (epoch 22 @ 4642 updates, score 0.43) (writing took 3.0978654709178954 seconds)\n",
            "2024-06-20 21:10:23 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2024-06-20 21:10:23 | INFO | train | epoch 022 | loss 0.484 | ppl 1.4 | wps 6074.1 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 4642 | lr 0.000928277 | gnorm 0.51 | train_wall 78 | gb_free 10.5 | wall 1900\n",
            "2024-06-20 21:10:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 023:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:10:23 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2024-06-20 21:10:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023: 100%|▉| 210/211 [01:18<00:00,  2.19it/s, loss=0.47, ppl=1.38, wps=7352024-06-20 21:11:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  10%|▋      | 5/51 [00:00<00:01, 38.73it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  18%|█▏     | 9/51 [00:00<00:01, 34.71it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  25%|█▌    | 13/51 [00:00<00:01, 32.49it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  33%|██    | 17/51 [00:00<00:01, 30.03it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  41%|██▍   | 21/51 [00:00<00:01, 28.25it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  47%|██▊   | 24/51 [00:00<00:01, 26.98it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  53%|███▏  | 27/51 [00:00<00:00, 25.54it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  59%|███▌  | 30/51 [00:01<00:00, 23.60it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  65%|███▉  | 33/51 [00:01<00:00, 20.23it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  71%|████▏ | 36/51 [00:01<00:00, 16.16it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  75%|████▍ | 38/51 [00:01<00:00, 13.82it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  78%|████▋ | 40/51 [00:02<00:00, 12.57it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  82%|████▉ | 42/51 [00:02<00:00, 11.45it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  86%|█████▏| 44/51 [00:02<00:00, 10.34it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  90%|█████▍| 46/51 [00:02<00:00,  8.96it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.41it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.71it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.68it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.12it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.04it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:11:46 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 0.409 | ppl 1.33 | wps 20719.1 | wpb 1556.3 | bsz 51.3 | num_updates 4853 | best_loss 0.409\n",
            "2024-06-20 21:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4853 updates\n",
            "2024-06-20 21:11:46 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint23.pt\n",
            "2024-06-20 21:11:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint23.pt\n",
            "2024-06-20 21:11:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint23.pt (epoch 23 @ 4853 updates, score 0.409) (writing took 2.9874294510809705 seconds)\n",
            "2024-06-20 21:11:49 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2024-06-20 21:11:49 | INFO | train | epoch 023 | loss 0.462 | ppl 1.38 | wps 6094.8 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 4853 | lr 0.000907872 | gnorm 0.518 | train_wall 78 | gb_free 10.4 | wall 1985\n",
            "2024-06-20 21:11:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 024:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:11:49 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2024-06-20 21:11:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024: 100%|▉| 210/211 [01:18<00:00,  2.86it/s, loss=0.412, ppl=1.33, wps=682024-06-20 21:13:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.92it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.54it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.63it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.65it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.34it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.22it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.92it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.18it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.57it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.33it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.77it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.04it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.91it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.78it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.63it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.37it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.76it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.79it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.26it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.18it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:13:12 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 0.408 | ppl 1.33 | wps 20736.8 | wpb 1556.3 | bsz 51.3 | num_updates 5064 | best_loss 0.408\n",
            "2024-06-20 21:13:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 5064 updates\n",
            "2024-06-20 21:13:12 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint24.pt\n",
            "2024-06-20 21:13:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint24.pt\n",
            "2024-06-20 21:13:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint24.pt (epoch 24 @ 5064 updates, score 0.408) (writing took 3.3245461649494246 seconds)\n",
            "2024-06-20 21:13:15 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2024-06-20 21:13:15 | INFO | train | epoch 024 | loss 0.407 | ppl 1.33 | wps 6069.2 | ups 2.45 | wpb 2474.1 | bsz 92.3 | num_updates 5064 | lr 0.000888757 | gnorm 0.386 | train_wall 78 | gb_free 10.5 | wall 2071\n",
            "2024-06-20 21:13:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 025:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:13:15 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2024-06-20 21:13:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025: 100%|▉| 210/211 [01:18<00:00,  2.80it/s, loss=0.396, ppl=1.32, wps=652024-06-20 21:14:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.86it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.54it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.58it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.63it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.40it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.25it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.90it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.15it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.57it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.29it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.74it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.01it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.91it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.76it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.64it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.37it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.75it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.79it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.28it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.17it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:14:38 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 0.41 | ppl 1.33 | wps 20720.2 | wpb 1556.3 | bsz 51.3 | num_updates 5275 | best_loss 0.408\n",
            "2024-06-20 21:14:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 5275 updates\n",
            "2024-06-20 21:14:38 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint25.pt\n",
            "2024-06-20 21:14:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint25.pt\n",
            "2024-06-20 21:14:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint25.pt (epoch 25 @ 5275 updates, score 0.41) (writing took 1.9088434119476005 seconds)\n",
            "2024-06-20 21:14:40 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2024-06-20 21:14:40 | INFO | train | epoch 025 | loss 0.391 | ppl 1.31 | wps 6177.1 | ups 2.5 | wpb 2474.1 | bsz 92.3 | num_updates 5275 | lr 0.000870801 | gnorm 0.399 | train_wall 77 | gb_free 10.4 | wall 2156\n",
            "2024-06-20 21:14:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 026:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:14:40 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2024-06-20 21:14:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026: 100%|▉| 210/211 [01:18<00:00,  2.96it/s, loss=0.372, ppl=1.29, wps=602024-06-20 21:15:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.80it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.45it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.56it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.61it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.40it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.27it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.92it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.18it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.60it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.32it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.77it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.03it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.90it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.77it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.37it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.76it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.79it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.24it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.15it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:16:02 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 0.399 | ppl 1.32 | wps 20703.6 | wpb 1556.3 | bsz 51.3 | num_updates 5486 | best_loss 0.399\n",
            "2024-06-20 21:16:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5486 updates\n",
            "2024-06-20 21:16:02 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint26.pt\n",
            "2024-06-20 21:16:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint26.pt\n",
            "2024-06-20 21:16:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint26.pt (epoch 26 @ 5486 updates, score 0.399) (writing took 3.366975412936881 seconds)\n",
            "2024-06-20 21:16:06 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2024-06-20 21:16:06 | INFO | train | epoch 026 | loss 0.369 | ppl 1.29 | wps 6063.3 | ups 2.45 | wpb 2474.1 | bsz 92.3 | num_updates 5486 | lr 0.00085389 | gnorm 0.358 | train_wall 78 | gb_free 10.3 | wall 2242\n",
            "2024-06-20 21:16:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 027:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:16:06 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2024-06-20 21:16:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027: 100%|▉| 210/211 [01:18<00:00,  3.95it/s, loss=0.345, ppl=1.27, wps=682024-06-20 21:17:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.98it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.46it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.54it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.57it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.34it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.18it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.84it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.12it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.55it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.30it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.77it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.00it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.89it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.77it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.36it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.75it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.27it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.17it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:17:28 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 0.389 | ppl 1.31 | wps 20705.7 | wpb 1556.3 | bsz 51.3 | num_updates 5697 | best_loss 0.389\n",
            "2024-06-20 21:17:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5697 updates\n",
            "2024-06-20 21:17:28 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint27.pt\n",
            "2024-06-20 21:17:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint27.pt\n",
            "2024-06-20 21:17:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint27.pt (epoch 27 @ 5697 updates, score 0.389) (writing took 3.7323274350492284 seconds)\n",
            "2024-06-20 21:17:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2024-06-20 21:17:32 | INFO | train | epoch 027 | loss 0.351 | ppl 1.28 | wps 6034.5 | ups 2.44 | wpb 2474.1 | bsz 92.3 | num_updates 5697 | lr 0.000837928 | gnorm 0.338 | train_wall 77 | gb_free 10.6 | wall 2328\n",
            "2024-06-20 21:17:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 028:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:17:32 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2024-06-20 21:17:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028: 100%|▉| 210/211 [01:18<00:00,  3.75it/s, loss=0.352, ppl=1.28, wps=692024-06-20 21:18:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  10%|▋      | 5/51 [00:00<00:01, 39.08it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  18%|█▏     | 9/51 [00:00<00:01, 34.91it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  25%|█▌    | 13/51 [00:00<00:01, 32.64it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  33%|██    | 17/51 [00:00<00:01, 30.11it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  41%|██▍   | 21/51 [00:00<00:01, 28.32it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  47%|██▊   | 24/51 [00:00<00:00, 27.03it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  53%|███▏  | 27/51 [00:00<00:00, 25.58it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  59%|███▌  | 30/51 [00:01<00:00, 23.64it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  65%|███▉  | 33/51 [00:01<00:00, 20.23it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  71%|████▏ | 36/51 [00:01<00:00, 16.16it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  75%|████▍ | 38/51 [00:01<00:00, 13.80it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  78%|████▋ | 40/51 [00:02<00:00, 12.55it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  82%|████▉ | 42/51 [00:02<00:00, 11.43it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  86%|█████▏| 44/51 [00:02<00:00, 10.31it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  90%|█████▍| 46/51 [00:02<00:00,  8.94it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.39it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.69it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.67it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.13it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.04it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:18:55 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 0.396 | ppl 1.32 | wps 20712.9 | wpb 1556.3 | bsz 51.3 | num_updates 5908 | best_loss 0.389\n",
            "2024-06-20 21:18:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5908 updates\n",
            "2024-06-20 21:18:55 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint28.pt\n",
            "2024-06-20 21:18:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint28.pt\n",
            "2024-06-20 21:18:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint28.pt (epoch 28 @ 5908 updates, score 0.396) (writing took 2.184054889017716 seconds)\n",
            "2024-06-20 21:18:57 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2024-06-20 21:18:57 | INFO | train | epoch 028 | loss 0.352 | ppl 1.28 | wps 6152.4 | ups 2.49 | wpb 2474.1 | bsz 92.3 | num_updates 5908 | lr 0.000822829 | gnorm 0.461 | train_wall 78 | gb_free 10.5 | wall 2413\n",
            "2024-06-20 21:18:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 029:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:18:57 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2024-06-20 21:18:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029: 100%|▉| 210/211 [01:18<00:00,  2.51it/s, loss=0.333, ppl=1.26, wps=652024-06-20 21:20:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.62it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.51it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.51it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.57it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.37it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.22it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.88it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.16it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.57it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.30it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.76it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.01it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.87it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.75it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.60it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.34it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.74it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.25it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.17it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:20:19 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 0.383 | ppl 1.3 | wps 20684.4 | wpb 1556.3 | bsz 51.3 | num_updates 6119 | best_loss 0.383\n",
            "2024-06-20 21:20:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 6119 updates\n",
            "2024-06-20 21:20:19 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint29.pt\n",
            "2024-06-20 21:20:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint29.pt\n",
            "2024-06-20 21:20:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint29.pt (epoch 29 @ 6119 updates, score 0.383) (writing took 3.7713090669130906 seconds)\n",
            "2024-06-20 21:20:23 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2024-06-20 21:20:23 | INFO | train | epoch 029 | loss 0.339 | ppl 1.26 | wps 6053.9 | ups 2.45 | wpb 2474.1 | bsz 92.3 | num_updates 6119 | lr 0.000808518 | gnorm 0.47 | train_wall 77 | gb_free 10.4 | wall 2499\n",
            "2024-06-20 21:20:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 030:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:20:23 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2024-06-20 21:20:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030: 100%|▉| 210/211 [01:16<00:00,  3.53it/s, loss=0.306, ppl=1.24, wps=622024-06-20 21:21:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  10%|▋      | 5/51 [00:00<00:01, 38.59it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  18%|█▏     | 9/51 [00:00<00:01, 34.55it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  25%|█▌    | 13/51 [00:00<00:01, 32.42it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  33%|██    | 17/51 [00:00<00:01, 29.94it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  41%|██▍   | 21/51 [00:00<00:01, 28.18it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  47%|██▊   | 24/51 [00:00<00:01, 26.88it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  53%|███▏  | 27/51 [00:00<00:00, 25.46it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  59%|███▌  | 30/51 [00:01<00:00, 23.52it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  65%|███▉  | 33/51 [00:01<00:00, 20.17it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  71%|████▏ | 36/51 [00:01<00:00, 16.12it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  75%|████▍ | 38/51 [00:01<00:00, 13.79it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  78%|████▋ | 40/51 [00:02<00:00, 12.53it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  82%|████▉ | 42/51 [00:02<00:00, 11.41it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  86%|█████▏| 44/51 [00:02<00:00, 10.30it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  90%|█████▍| 46/51 [00:02<00:00,  8.92it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.39it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.68it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.66it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.11it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.04it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:21:46 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 0.379 | ppl 1.3 | wps 20665.1 | wpb 1556.3 | bsz 51.3 | num_updates 6330 | best_loss 0.379\n",
            "2024-06-20 21:21:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 6330 updates\n",
            "2024-06-20 21:21:46 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint30.pt\n",
            "2024-06-20 21:21:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint30.pt\n",
            "2024-06-20 21:21:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint30.pt (epoch 30 @ 6330 updates, score 0.379) (writing took 4.464674362097867 seconds)\n",
            "2024-06-20 21:21:50 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2024-06-20 21:21:50 | INFO | train | epoch 030 | loss 0.31 | ppl 1.24 | wps 5981.7 | ups 2.42 | wpb 2474.1 | bsz 92.3 | num_updates 6330 | lr 0.000794929 | gnorm 0.348 | train_wall 78 | gb_free 10.7 | wall 2587\n",
            "2024-06-20 21:21:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 031:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:21:50 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2024-06-20 21:21:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031: 100%|▉| 210/211 [01:18<00:00,  2.94it/s, loss=0.314, ppl=1.24, wps=692024-06-20 21:23:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.94it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.50it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.59it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.62it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.39it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.24it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.88it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.15it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.56it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.27it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.76it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.02it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.88it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.76it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.36it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.75it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.22it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.14it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:23:13 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 0.375 | ppl 1.3 | wps 20667.4 | wpb 1556.3 | bsz 51.3 | num_updates 6541 | best_loss 0.375\n",
            "2024-06-20 21:23:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6541 updates\n",
            "2024-06-20 21:23:13 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint31.pt\n",
            "2024-06-20 21:23:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint31.pt\n",
            "2024-06-20 21:23:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint31.pt (epoch 31 @ 6541 updates, score 0.375) (writing took 3.226608199067414 seconds)\n",
            "2024-06-20 21:23:16 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2024-06-20 21:23:16 | INFO | train | epoch 031 | loss 0.325 | ppl 1.25 | wps 6068 | ups 2.45 | wpb 2474.1 | bsz 92.3 | num_updates 6541 | lr 0.000782002 | gnorm 0.503 | train_wall 78 | gb_free 10.4 | wall 2673\n",
            "2024-06-20 21:23:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 032:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:23:17 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2024-06-20 21:23:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032: 100%|▉| 210/211 [01:18<00:00,  2.54it/s, loss=0.288, ppl=1.22, wps=672024-06-20 21:24:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.04it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 34.90it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.13it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.26it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.07it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 26.95it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.65it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 23.97it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.40it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.17it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.68it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.94it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.82it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.71it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.57it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.32it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.72it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.77it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.21it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.13it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:24:39 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 0.37 | ppl 1.29 | wps 20566.2 | wpb 1556.3 | bsz 51.3 | num_updates 6752 | best_loss 0.37\n",
            "2024-06-20 21:24:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6752 updates\n",
            "2024-06-20 21:24:39 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint32.pt\n",
            "2024-06-20 21:24:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint32.pt\n",
            "2024-06-20 21:24:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint32.pt (epoch 32 @ 6752 updates, score 0.37) (writing took 3.895123400958255 seconds)\n",
            "2024-06-20 21:24:43 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2024-06-20 21:24:43 | INFO | train | epoch 032 | loss 0.288 | ppl 1.22 | wps 6020.2 | ups 2.43 | wpb 2474.1 | bsz 92.3 | num_updates 6752 | lr 0.000769686 | gnorm 0.306 | train_wall 78 | gb_free 10.3 | wall 2759\n",
            "2024-06-20 21:24:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 033:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:24:43 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2024-06-20 21:24:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033: 100%|▉| 210/211 [01:18<00:00,  3.24it/s, loss=0.281, ppl=1.21, wps=622024-06-20 21:26:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.44it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.29it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.30it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.31it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.10it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 26.96it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.63it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 23.90it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.33it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.13it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.65it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.93it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.83it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.70it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.58it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.33it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.72it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.77it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.25it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.15it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:26:06 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 0.377 | ppl 1.3 | wps 20589.2 | wpb 1556.3 | bsz 51.3 | num_updates 6963 | best_loss 0.37\n",
            "2024-06-20 21:26:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6963 updates\n",
            "2024-06-20 21:26:06 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint33.pt\n",
            "2024-06-20 21:26:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint33.pt\n",
            "2024-06-20 21:26:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint33.pt (epoch 33 @ 6963 updates, score 0.377) (writing took 2.1365449170116335 seconds)\n",
            "2024-06-20 21:26:08 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2024-06-20 21:26:08 | INFO | train | epoch 033 | loss 0.288 | ppl 1.22 | wps 6140 | ups 2.48 | wpb 2474.1 | bsz 92.3 | num_updates 6963 | lr 0.000757935 | gnorm 0.372 | train_wall 78 | gb_free 10.4 | wall 2844\n",
            "2024-06-20 21:26:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 034:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:26:08 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2024-06-20 21:26:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034: 100%|▉| 210/211 [01:18<00:00,  2.30it/s, loss=0.274, ppl=1.21, wps=762024-06-20 21:27:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.81it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.51it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.61it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.62it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.36it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.19it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.85it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.12it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.49it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.26it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.73it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.00it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.88it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.77it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.37it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.75it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.79it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.22it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.14it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:27:31 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 0.377 | ppl 1.3 | wps 20661.7 | wpb 1556.3 | bsz 51.3 | num_updates 7174 | best_loss 0.37\n",
            "2024-06-20 21:27:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 7174 updates\n",
            "2024-06-20 21:27:31 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint34.pt\n",
            "2024-06-20 21:27:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint34.pt\n",
            "2024-06-20 21:27:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint34.pt (epoch 34 @ 7174 updates, score 0.377) (writing took 1.8665790819795802 seconds)\n",
            "2024-06-20 21:27:33 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2024-06-20 21:27:33 | INFO | train | epoch 034 | loss 0.278 | ppl 1.21 | wps 6167.7 | ups 2.49 | wpb 2474.1 | bsz 92.3 | num_updates 7174 | lr 0.000746705 | gnorm 0.379 | train_wall 78 | gb_free 10.6 | wall 2929\n",
            "2024-06-20 21:27:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 035:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:27:33 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2024-06-20 21:27:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035: 100%|▉| 210/211 [01:18<00:00,  2.83it/s, loss=0.257, ppl=1.2, wps=6792024-06-20 21:28:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.80it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.31it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.48it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.55it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.34it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.20it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.83it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.11it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.53it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.28it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.75it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.01it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.89it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.76it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.36it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.74it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.26it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.16it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:28:55 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 0.394 | ppl 1.31 | wps 20688 | wpb 1556.3 | bsz 51.3 | num_updates 7385 | best_loss 0.37\n",
            "2024-06-20 21:28:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 7385 updates\n",
            "2024-06-20 21:28:55 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint35.pt\n",
            "2024-06-20 21:28:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint35.pt\n",
            "2024-06-20 21:28:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint35.pt (epoch 35 @ 7385 updates, score 0.394) (writing took 2.6353628190699965 seconds)\n",
            "2024-06-20 21:28:58 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2024-06-20 21:28:58 | INFO | train | epoch 035 | loss 0.259 | ppl 1.2 | wps 6130.8 | ups 2.48 | wpb 2474.1 | bsz 92.3 | num_updates 7385 | lr 0.000735961 | gnorm 0.298 | train_wall 77 | gb_free 10.4 | wall 3014\n",
            "2024-06-20 21:28:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 036:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:28:58 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2024-06-20 21:28:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036: 100%|▉| 210/211 [01:18<00:00,  3.03it/s, loss=0.252, ppl=1.19, wps=642024-06-20 21:30:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.55it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.28it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.39it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.40it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.15it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 26.99it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.72it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.06it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.47it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.21it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.69it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.98it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.86it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.74it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.35it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.74it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.23it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.15it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:30:21 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 0.378 | ppl 1.3 | wps 20635.7 | wpb 1556.3 | bsz 51.3 | num_updates 7596 | best_loss 0.37\n",
            "2024-06-20 21:30:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7596 updates\n",
            "2024-06-20 21:30:21 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint36.pt\n",
            "2024-06-20 21:30:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint36.pt\n",
            "2024-06-20 21:30:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint36.pt (epoch 36 @ 7596 updates, score 0.378) (writing took 2.5741277730558068 seconds)\n",
            "2024-06-20 21:30:23 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2024-06-20 21:30:23 | INFO | train | epoch 036 | loss 0.253 | ppl 1.19 | wps 6113 | ups 2.47 | wpb 2474.1 | bsz 92.3 | num_updates 7596 | lr 0.000725667 | gnorm 0.318 | train_wall 78 | gb_free 10.4 | wall 3100\n",
            "2024-06-20 21:30:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 037:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:30:23 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2024-06-20 21:30:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037: 100%|▉| 210/211 [01:18<00:00,  4.05it/s, loss=0.243, ppl=1.18, wps=642024-06-20 21:31:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.40it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.19it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.35it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.38it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.22it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.15it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.83it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.12it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.55it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.29it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.73it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.00it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.86it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.73it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.57it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.33it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.73it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.77it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.22it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.15it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:31:46 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 0.38 | ppl 1.3 | wps 20637 | wpb 1556.3 | bsz 51.3 | num_updates 7807 | best_loss 0.37\n",
            "2024-06-20 21:31:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7807 updates\n",
            "2024-06-20 21:31:46 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint37.pt\n",
            "2024-06-20 21:31:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint37.pt\n",
            "2024-06-20 21:31:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint37.pt (epoch 37 @ 7807 updates, score 0.38) (writing took 1.9355659099528566 seconds)\n",
            "2024-06-20 21:31:48 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2024-06-20 21:31:48 | INFO | train | epoch 037 | loss 0.236 | ppl 1.18 | wps 6173.7 | ups 2.5 | wpb 2474.1 | bsz 92.3 | num_updates 7807 | lr 0.000715794 | gnorm 0.292 | train_wall 78 | gb_free 10.4 | wall 3184\n",
            "2024-06-20 21:31:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 038:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:31:48 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2024-06-20 21:31:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038: 100%|▉| 210/211 [01:18<00:00,  3.40it/s, loss=0.241, ppl=1.18, wps=642024-06-20 21:33:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.90it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.44it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.57it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.65it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.40it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.26it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.93it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.18it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.58it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.27it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.76it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.03it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.91it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.77it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.62it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.35it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.73it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.24it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.15it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:33:11 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 0.382 | ppl 1.3 | wps 20688.1 | wpb 1556.3 | bsz 51.3 | num_updates 8018 | best_loss 0.37\n",
            "2024-06-20 21:33:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 8018 updates\n",
            "2024-06-20 21:33:11 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint38.pt\n",
            "2024-06-20 21:33:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint38.pt\n",
            "2024-06-20 21:33:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint38.pt (epoch 38 @ 8018 updates, score 0.382) (writing took 1.9228905709460378 seconds)\n",
            "2024-06-20 21:33:13 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2024-06-20 21:33:13 | INFO | train | epoch 038 | loss 0.23 | ppl 1.17 | wps 6159.2 | ups 2.49 | wpb 2474.1 | bsz 92.3 | num_updates 8018 | lr 0.000706313 | gnorm 0.316 | train_wall 78 | gb_free 10.5 | wall 3269\n",
            "2024-06-20 21:33:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 039:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:33:13 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2024-06-20 21:33:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039: 100%|▉| 210/211 [01:18<00:00,  2.79it/s, loss=0.231, ppl=1.17, wps=612024-06-20 21:34:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.05it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.14it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.31it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.36it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.19it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.06it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.74it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.05it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.49it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.22it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.69it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.96it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.84it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.72it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.60it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.34it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.73it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.26it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.16it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:34:35 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 0.4 | ppl 1.32 | wps 20635.8 | wpb 1556.3 | bsz 51.3 | num_updates 8229 | best_loss 0.37\n",
            "2024-06-20 21:34:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 8229 updates\n",
            "2024-06-20 21:34:35 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint39.pt\n",
            "2024-06-20 21:34:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint39.pt\n",
            "2024-06-20 21:34:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint39.pt (epoch 39 @ 8229 updates, score 0.4) (writing took 1.9586882289731875 seconds)\n",
            "2024-06-20 21:34:37 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2024-06-20 21:34:37 | INFO | train | epoch 039 | loss 0.223 | ppl 1.17 | wps 6168.8 | ups 2.49 | wpb 2474.1 | bsz 92.3 | num_updates 8229 | lr 0.000697199 | gnorm 0.383 | train_wall 78 | gb_free 10.4 | wall 3354\n",
            "2024-06-20 21:34:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 040:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:34:37 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2024-06-20 21:34:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040: 100%|▉| 210/211 [01:18<00:00,  2.73it/s, loss=0.201, ppl=1.15, wps=732024-06-20 21:35:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  10%|▋      | 5/51 [00:00<00:01, 38.79it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  18%|█▏     | 9/51 [00:00<00:01, 34.76it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  25%|█▌    | 13/51 [00:00<00:01, 32.53it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  33%|██    | 17/51 [00:00<00:01, 30.04it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  41%|██▍   | 21/51 [00:00<00:01, 28.27it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  47%|██▊   | 24/51 [00:00<00:01, 26.98it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  53%|███▏  | 27/51 [00:00<00:00, 25.53it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  59%|███▌  | 30/51 [00:01<00:00, 23.54it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  65%|███▉  | 33/51 [00:01<00:00, 20.22it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  71%|████▏ | 36/51 [00:01<00:00, 16.13it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  75%|████▍ | 38/51 [00:01<00:00, 13.81it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  78%|████▋ | 40/51 [00:02<00:00, 12.56it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  82%|████▉ | 42/51 [00:02<00:00, 11.46it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  86%|█████▏| 44/51 [00:02<00:00, 10.34it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  90%|█████▍| 46/51 [00:02<00:00,  8.96it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.42it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.71it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.68it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.12it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.05it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:36:00 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 0.395 | ppl 1.32 | wps 20725.6 | wpb 1556.3 | bsz 51.3 | num_updates 8440 | best_loss 0.37\n",
            "2024-06-20 21:36:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 8440 updates\n",
            "2024-06-20 21:36:00 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint40.pt\n",
            "2024-06-20 21:36:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint40.pt\n",
            "2024-06-20 21:36:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint40.pt (epoch 40 @ 8440 updates, score 0.395) (writing took 2.003737539984286 seconds)\n",
            "2024-06-20 21:36:02 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2024-06-20 21:36:02 | INFO | train | epoch 040 | loss 0.203 | ppl 1.15 | wps 6161.2 | ups 2.49 | wpb 2474.1 | bsz 92.3 | num_updates 8440 | lr 0.000688428 | gnorm 0.283 | train_wall 78 | gb_free 10.5 | wall 3438\n",
            "2024-06-20 21:36:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 041:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:36:02 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2024-06-20 21:36:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041: 100%|▉| 210/211 [01:18<00:00,  2.47it/s, loss=0.197, ppl=1.15, wps=702024-06-20 21:37:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.39it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.25it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.45it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.52it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.33it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.20it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.88it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.10it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.52it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.25it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.71it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.97it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.85it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.71it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.58it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.34it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.73it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.77it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.25it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.15it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:37:25 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 0.41 | ppl 1.33 | wps 20636.5 | wpb 1556.3 | bsz 51.3 | num_updates 8651 | best_loss 0.37\n",
            "2024-06-20 21:37:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 8651 updates\n",
            "2024-06-20 21:37:25 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint41.pt\n",
            "2024-06-20 21:37:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint41.pt\n",
            "2024-06-20 21:37:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint41.pt (epoch 41 @ 8651 updates, score 0.41) (writing took 2.210573432035744 seconds)\n",
            "2024-06-20 21:37:27 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2024-06-20 21:37:27 | INFO | train | epoch 041 | loss 0.198 | ppl 1.15 | wps 6146.5 | ups 2.48 | wpb 2474.1 | bsz 92.3 | num_updates 8651 | lr 0.000679981 | gnorm 0.292 | train_wall 78 | gb_free 10.4 | wall 3523\n",
            "2024-06-20 21:37:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 042:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:37:27 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2024-06-20 21:37:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042: 100%|▉| 210/211 [01:18<00:00,  3.75it/s, loss=0.183, ppl=1.14, wps=652024-06-20 21:38:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 38.93it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 34.95it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.12it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.18it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.01it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 26.89it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.58it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 23.90it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.34it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.13it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.64it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.92it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.80it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.67it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.53it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.30it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.70it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.75it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.21it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.15it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:38:50 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 0.403 | ppl 1.32 | wps 20541.6 | wpb 1556.3 | bsz 51.3 | num_updates 8862 | best_loss 0.37\n",
            "2024-06-20 21:38:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8862 updates\n",
            "2024-06-20 21:38:50 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint42.pt\n",
            "2024-06-20 21:38:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint42.pt\n",
            "2024-06-20 21:38:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint42.pt (epoch 42 @ 8862 updates, score 0.403) (writing took 2.8515039819758385 seconds)\n",
            "2024-06-20 21:38:53 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2024-06-20 21:38:53 | INFO | train | epoch 042 | loss 0.191 | ppl 1.14 | wps 6089.8 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 8862 | lr 0.000671837 | gnorm 0.293 | train_wall 78 | gb_free 10.4 | wall 3609\n",
            "2024-06-20 21:38:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 043:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:38:53 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2024-06-20 21:38:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043: 100%|▉| 210/211 [01:19<00:00,  2.39it/s, loss=0.191, ppl=1.14, wps=642024-06-20 21:40:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.64it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.36it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.56it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.62it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.41it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.25it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.93it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.19it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.54it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.24it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.70it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 12.95it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.85it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.73it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.60it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.34it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.73it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.77it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.21it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.13it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:40:16 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 0.412 | ppl 1.33 | wps 20634.8 | wpb 1556.3 | bsz 51.3 | num_updates 9073 | best_loss 0.37\n",
            "2024-06-20 21:40:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 9073 updates\n",
            "2024-06-20 21:40:16 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint43.pt\n",
            "2024-06-20 21:40:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint43.pt\n",
            "2024-06-20 21:40:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint43.pt (epoch 43 @ 9073 updates, score 0.412) (writing took 2.377358219004236 seconds)\n",
            "2024-06-20 21:40:18 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2024-06-20 21:40:18 | INFO | train | epoch 043 | loss 0.184 | ppl 1.14 | wps 6105.1 | ups 2.47 | wpb 2474.1 | bsz 92.3 | num_updates 9073 | lr 0.000663979 | gnorm 0.331 | train_wall 78 | gb_free 10.3 | wall 3694\n",
            "2024-06-20 21:40:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 211\n",
            "epoch 044:   0%|                                        | 0/211 [00:00<?, ?it/s]2024-06-20 21:40:18 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2024-06-20 21:40:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044: 100%|▉| 210/211 [01:18<00:00,  2.76it/s, loss=0.183, ppl=1.14, wps=672024-06-20 21:41:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0%|               | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:   8%|▌      | 4/51 [00:00<00:01, 39.97it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  16%|█      | 8/51 [00:00<00:01, 35.60it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  24%|█▍    | 12/51 [00:00<00:01, 32.58it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  31%|█▉    | 16/51 [00:00<00:01, 30.54it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  39%|██▎   | 20/51 [00:00<00:01, 28.28it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  45%|██▋   | 23/51 [00:00<00:01, 27.11it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  51%|███   | 26/51 [00:00<00:00, 25.79it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  57%|███▍  | 29/51 [00:01<00:00, 24.08it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  63%|███▊  | 32/51 [00:01<00:00, 21.53it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  69%|████  | 35/51 [00:01<00:00, 17.25it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  73%|████▎ | 37/51 [00:01<00:00, 14.74it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  76%|████▌ | 39/51 [00:01<00:00, 13.02it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  80%|████▊ | 41/51 [00:02<00:00, 11.90it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  84%|█████ | 43/51 [00:02<00:00, 10.75it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  88%|█████▎| 45/51 [00:02<00:00,  9.61it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  92%|█████▌| 47/51 [00:02<00:00,  8.36it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  94%|█████▋| 48/51 [00:03<00:00,  7.74it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  96%|█████▊| 49/51 [00:03<00:00,  6.78it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  98%|█████▉| 50/51 [00:03<00:00,  6.21it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset: 100%|██████| 51/51 [00:03<00:00,  5.14it/s]\u001b[A\n",
            "                                                                                \u001b[A2024-06-20 21:41:41 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 0.416 | ppl 1.33 | wps 20653.1 | wpb 1556.3 | bsz 51.3 | num_updates 9284 | best_loss 0.37\n",
            "2024-06-20 21:41:41 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 12 runs\n",
            "2024-06-20 21:41:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 9284 updates\n",
            "2024-06-20 21:41:41 | INFO | fairseq.trainer | Saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint44.pt\n",
            "2024-06-20 21:41:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/g/gabays/FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint44.pt\n",
            "2024-06-20 21:41:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint44.pt (epoch 44 @ 9284 updates, score 0.416) (writing took 2.6725505960639566 seconds)\n",
            "2024-06-20 21:41:44 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2024-06-20 21:41:44 | INFO | train | epoch 044 | loss 0.181 | ppl 1.13 | wps 6081.6 | ups 2.46 | wpb 2474.1 | bsz 92.3 | num_updates 9284 | lr 0.000656391 | gnorm 0.33 | train_wall 78 | gb_free 10.6 | wall 3780\n",
            "2024-06-20 21:41:44 | INFO | fairseq_cli.train | done training in 3777.8 seconds\n"
          ]
        }
      ],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict1000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_1000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict1000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65_zng4wKW9X"
      },
      "source": [
        "We now train a model with a vocab of 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRuUuPBlhxQz"
      },
      "outputs": [],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict2000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_2000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict2000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeyaJaT9KW9X"
      },
      "source": [
        "We now train a model with a vocab of 3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWlEgWckKW9X"
      },
      "outputs": [],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict3000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_3000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict3000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lqm338KKW9X"
      },
      "source": [
        "We now train a model with a vocab of 4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXwR3iooKW9X"
      },
      "outputs": [],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict4000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_4000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict4000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee_ycyjbpQrx"
      },
      "source": [
        "## III Testing\n",
        "\n",
        "We will need a few functions. One for \"pasting\" the BPEs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4QxQWS-qNcC"
      },
      "outputs": [],
      "source": [
        "def decode_sp(list_sents):\n",
        "    return [''.join(sent).replace(' ', '').replace('▁', ' ').strip() for sent in list_sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L30TvaJzqPDP"
      },
      "source": [
        "One to extract the hypothesis in the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqXpCmX2pjFE"
      },
      "outputs": [],
      "source": [
        "def extract_hypothesis(filename):\n",
        "    outputs = []\n",
        "    with open(filename) as fp:\n",
        "        for line in fp:\n",
        "            # seulement les lignes qui commencet par H- (pour Hypothèse)\n",
        "            if 'H-' in line:\n",
        "                # prendre la 3ème colonne (c'est-à-dire l'indice 2)\n",
        "                outputs.append(line.strip().split('\\t')[2])\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya-Kvv_cKW9d"
      },
      "source": [
        "### III.a 1000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhNHQaCyKW9d"
      },
      "outputs": [],
      "source": [
        "import sentencepiece, os\n",
        "!mkdir -p $DIRECTORY/dev\n",
        "\n",
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_1000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict1000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_1000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yD-OOqpKW9d"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dOmZRSIKW9d",
        "outputId": "c9218563-2ad1-433b-f213-ee408d0f4021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-20 21:49:57 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'FreEMnorm/data/data_norm_bin_1000', 'source_lang': 'src', 'target_lang': 'trg', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-06-20 21:49:57 | INFO | fairseq.tasks.translation | [src] dictionary: 952 types\n",
            "2024-06-20 21:49:57 | INFO | fairseq.tasks.translation | [trg] dictionary: 952 types\n",
            "2024-06-20 21:49:57 | INFO | fairseq_cli.interactive | loading model(s) from FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint_best.pt\n",
            "2024-06-20 21:50:01 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-06-20 21:50:01 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2024-06-20 22:00:34 | INFO | fairseq_cli.interactive | Total time: 636.929 seconds; translation time: 591.704\n"
          ]
        }
      ],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzMZcUvaKW9e"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwB2arlTKW9e",
        "outputId": "f9468a49-9565-441e-d818-5fd443bf6a15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BLEU = 52.51 77.4/59.4/46.2/35.8 (BP = 1.000 ratio = 1.003 hyp_len = 82838 ref_len = 82598)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMgwM1UPKW9e"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDAF92GqKW9e"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z_cipI4KW9e"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQywcDxMKW9e"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR-DObUsKW9e"
      },
      "source": [
        "Word accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUhPBeHrKW9e"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK2KaeVEKW9e"
      },
      "source": [
        "### III.a 2000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3IVBrg_KW9e"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $DIRECTORY/dev\n",
        "\n",
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_2000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict2000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_2000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfo82HEyKW9e"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7gr2wVUKW9e"
      },
      "outputs": [],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNJgdP_UKW9e"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hFN26FyKW9f"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Xbju3LKW9f"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoCmtNnGKW9f"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYgictVjKW9f"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIygr2ANKW9f"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj6lCWULKW9f"
      },
      "source": [
        "Word accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgbAo2DrKW9f"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6tZ5euJKW9f"
      },
      "source": [
        "### III.b 3000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmkJ_u8nqjpY"
      },
      "outputs": [],
      "source": [
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_3000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict3000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_3000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbuUYD7aKW9f"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrK73j7isbrs"
      },
      "outputs": [],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4aNn3ZNqjMV"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENN0ebiIrU17"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "#BLEU\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4C0Cd3NKW9f"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBTqay7rKW9f"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VRQLXeWKW9f"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6jdGawvKW9g"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVJ0P2YqKW9g"
      },
      "source": [
        "Word accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DclTFxjrhqY"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8KaNVGXKW9g"
      },
      "source": [
        "### III.b 4000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFeQ9viPKW9g"
      },
      "outputs": [],
      "source": [
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_4000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict4000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_4000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GDE2DeHKW9g"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz0XgzDOKW9g"
      },
      "outputs": [],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F86u5uJxKW9g"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd-TbuyOKW9g"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "#BLEU\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txJqQJO9KW9g"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au2tPqOKKW9g"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL8hlhuIKW9g"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SBrJ0w5KW9g"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk5aG1uyKW9g"
      },
      "source": [
        "Word accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ3LtYOxKW9g"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}