{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soniasol/test_normalisation_2/blob/main/notebooks/Train_norm_model_Sonia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIVO_5Oq3500"
      },
      "source": [
        "# Train a normaliser for French\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gabays/32M7131/blob/main/Cours_04/Cours04.ipynb)\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licence Creative Commons\" style=\"border-width:0;float:right;\\\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a>\n",
        "\n",
        "Simon Gabay (UniGE), Rachel Bawden (INRIA Paris)\n",
        "\n",
        "<img width=\"30px\" style=\"float:left\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"/>  Specific requirements for Colab users are signaled with this sign."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epurIP9mKdN5"
      },
      "source": [
        "**Sonia's note**: If working with HPC, create a virtual environment and activate it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS9in_4_LZ97"
      },
      "source": [
        "## I. Preparing the experiment\n",
        "\n",
        "You might need first to clean your env (OOD user in Geneva)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFm2yi8hJwPO"
      },
      "source": [
        "**Sonia's note**: about the following command\n",
        "\n",
        "1. !pip freeze --user --exclude-editable: Lists all installed Python packages for the current user, excluding editable packages.\n",
        "2. | xargs pip uninstall -y: Uninstalls all those packages without requiring confirmation (-y flag).\n",
        "3. !pip list: Lists the remaining installed packages after uninstallation.\n",
        "\n",
        "This is useful for cleaning up user-installed packages before reinstalling only necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dXFjCEFMKW9T",
        "outputId": "b14f269f-f38a-4381-a44b-c2ce1a5b0318"
      },
      "outputs": [],
      "source": [
        "!pip freeze --user --exclude-editable | xargs pip uninstall -y\n",
        "!pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD2irpZSKW9U"
      },
      "source": [
        "Installing required packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrlM-xQpK349"
      },
      "source": [
        "**Sonia's note**: about the following command\n",
        "\n",
        "We install the following Python packages with specific versions:\n",
        "\n",
        "1. fairseq==0.12.2: A sequence modeling toolkit for training custom neural networks.\n",
        "2. sentencepiece: A subword tokenization library.\n",
        "3. sacrebleu: A library for BLEU score calculation in machine translation.\n",
        "4. omegaconf==2.0.5: A hierarchical configuration library for Python.\n",
        "5. gdown==4.2.0: A utility for downloading files from Google Drive.\n",
        "6. tensorboardX: A TensorBoard logger for PyTorch.\n",
        "7. numpy==1.25.2: A fundamental numerical computing library.\n",
        "\n",
        "This ensures the correct dependencies for working with fairseq."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rG3o78iI30sR",
        "outputId": "1c0735e6-57d0-4959-d816-bb69915aa4ed"
      },
      "outputs": [],
      "source": [
        "!pip install fairseq==0.12.2 sentencepiece sacrebleu omegaconf==2.0.5 gdown==4.2.0 tensorboardX numpy==1.25.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meKj3h29KW9U"
      },
      "source": [
        "### I.a Colab vs other tools\n",
        "\n",
        "<img width=\"30px\" style=\"float:left\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"/>  Colab users:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-cheDZ4KW9U"
      },
      "outputs": [],
      "source": [
        "DIRECTORY=\"/content/FreEMnorm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWyiyh0HLrQN"
      },
      "source": [
        "**Sonia's note**: about the above command\n",
        "\n",
        "We set an environment variable DIRECTORY with the path \"/content/FreEMnorm\". This is commonly used to define a working directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TCc3zHOKW9U"
      },
      "source": [
        "Non colab users:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5_HRJ3OKW9U"
      },
      "outputs": [],
      "source": [
        "DIRECTORY=\"FreEMnorm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZoNmqSlKhSu"
      },
      "source": [
        "### I.b Retrieving data\n",
        "We download the repo (`dev` branch only for now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "79e9t2dvJWZe",
        "outputId": "dc9c678f-f443-431d-8e60-1623d593515e"
      },
      "outputs": [],
      "source": [
        "#delete if there is an older version of the repo\n",
        "!rm -rf $DIRECTORY\n",
        "# cloning dev branch\n",
        "!git clone -b dev https://github.com/FreEM-corpora/FreEMnorm.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK4_u83wMhE-"
      },
      "source": [
        "**Sonia's note**: about the above command\n",
        "\n",
        "We use the -b dev option to clone the development (dev) branch instead of the default main or master.\n",
        "Hovewer, I'll use my own data. No need for this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gHRyxOoKted"
      },
      "source": [
        "### I.c If you want to redo the split and the creation of the training data\n",
        "\n",
        "We can split the corpus and create a file with the data in the requires format.\n",
        "\n",
        "<img width=\"30px\" style=\"float:left\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"/> Add `-c` to the last command (`split_to_src_trg.py`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Sonia's note**: about the following command\n",
        "\n",
        "I'll use my own data. No need for this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzRsHEzd4WU3"
      },
      "outputs": [],
      "source": [
        "#Remove previously split directory if there is any\n",
        "!rm -rf $DIRECTORY/split/ $DIRECTORY/data/\n",
        "!echo \"Split and Data directories have been removed\"\n",
        "#We split the corpus into train/test/dev\n",
        "!python $DIRECTORY/split.py\n",
        "#We turn the split into the requires format\n",
        "!python $DIRECTORY/split_to_src_trg.py\n",
        "#Colab users should comment the previous one and use the following\n",
        "#!python /content/FreEMnorm/split_to_src_trg.py -c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLr0uJSPLmd7"
      },
      "source": [
        "## II. Traing a model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeQzsF3Lo9Qt"
      },
      "source": [
        "### II.a Preprocessing\n",
        "\n",
        "We will need a few functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H1PQJScKW9V"
      },
      "outputs": [],
      "source": [
        "# Read a file per line\n",
        "def read_file(filename):\n",
        "  list_sents = []\n",
        "  with open(filename) as fp:\n",
        "    for line in fp:\n",
        "      list_sents.append(line.strip())\n",
        "  return list_sents\n",
        "\n",
        "#write a file per line\n",
        "def write_file(list_sents, filename):\n",
        "    with open(filename, 'w') as fp:\n",
        "        for sent in list_sents:\n",
        "            fp.write(sent + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUDgRxGVKW9V"
      },
      "source": [
        "We will need various sizes of vocabulary: 2000, 3000, 4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIMXi--YLuHG"
      },
      "outputs": [],
      "source": [
        "import sentencepiece\n",
        "import os\n",
        "\n",
        "!rm -rf $DIRECTORY/data/vocabulary.src-trg\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_1000\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_2000\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_3000\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_4000\n",
        "\n",
        "# We make a big file with all the data\n",
        "!cat $DIRECTORY/data/* > $DIRECTORY/data/vocabulary.src-trg\n",
        "\n",
        "# 1000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_1000\"),\n",
        "                               vocab_size=1000)\n",
        "\n",
        "# 2000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_2000\"),\n",
        "                               vocab_size=2000)\n",
        "\n",
        "#3000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_3000\"),\n",
        "                               vocab_size=3000)\n",
        "\n",
        "#4000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_4000\"),\n",
        "                               vocab_size=4000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYweZ4Cmfet1"
      },
      "source": [
        "We prepare the various datasets with 1000 wods vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AnAUlyuKW9V",
        "outputId": "ba0fab5f-2c95-4e24-b2a6-59d5200742e7"
      },
      "outputs": [],
      "source": [
        "#Loading datasets\n",
        "train_src = read_file(os.path.join(DIRECTORY,'data/train.src'))\n",
        "train_trg = read_file(os.path.join(DIRECTORY,'data/train.trg'))\n",
        "dev_src = read_file(os.path.join(DIRECTORY,'data/dev.src'))\n",
        "dev_trg = read_file(os.path.join(DIRECTORY,'data/dev.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "\n",
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_1000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp1000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp1000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp1000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp1000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp1000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp1000.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpu6eFkJKW9W"
      },
      "source": [
        "2000 words vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q4vTqi6fiCl",
        "outputId": "ed32c510-8884-41b5-bb25-e023ab25a9af"
      },
      "outputs": [],
      "source": [
        "#Loading datasets\n",
        "train_src = read_file(os.path.join(DIRECTORY,'data/train.src'))\n",
        "train_trg = read_file(os.path.join(DIRECTORY,'data/train.trg'))\n",
        "dev_src = read_file(os.path.join(DIRECTORY,'data/dev.src'))\n",
        "dev_trg = read_file(os.path.join(DIRECTORY,'data/dev.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "\n",
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_2000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp2000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp2000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp2000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp2000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp2000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp2000.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtjAPohKW9W"
      },
      "source": [
        "3000 words vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ML_XId-KW9W",
        "outputId": "c8c8c7b0-0a52-4397-e0c8-55eabd31d574"
      },
      "outputs": [],
      "source": [
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_3000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp3000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp3000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp3000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp3000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp3000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp3000.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QThTgqDKW9W"
      },
      "source": [
        "4000 words vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XupMx_7SKW9W",
        "outputId": "387a20b4-4d34-41ad-b27b-f18b6d64b1bc"
      },
      "outputs": [],
      "source": [
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_4000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp4000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp4000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp4000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp4000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp4000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp4000.trg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngPU1B3wg4hG",
        "outputId": "0ab4cc9d-4976-44fe-97e7-566ef72d12d0"
      },
      "outputs": [],
      "source": [
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_1000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp1000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp1000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp1000 \\\n",
        "                    --joined-dictionary\n",
        "\n",
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_2000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp2000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp2000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp2000 \\\n",
        "                    --joined-dictionary\n",
        "\n",
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_3000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp3000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp3000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp3000 \\\n",
        "                    --joined-dictionary\n",
        "\n",
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_4000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp4000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp4000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp4000 \\\n",
        "                    --joined-dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yqxh3vSnZSq"
      },
      "source": [
        "### II.b Training\n",
        "\n",
        "We now train a model with a vocab of 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "U5X-AMlzKW9X",
        "outputId": "5651676c-2cec-405d-e9c2-cfd8ab00782d"
      },
      "outputs": [],
      "source": [
        "!pip uninstall numpy\n",
        "!pip install numpy==1.25.2\n",
        "import numpyy\n",
        "numpy.version.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9T583f25KW9X",
        "outputId": "62cbac37-e7bd-4cc3-c694-58569d75d7f9"
      },
      "outputs": [],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict1000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_1000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict1000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65_zng4wKW9X"
      },
      "source": [
        "We now train a model with a vocab of 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRuUuPBlhxQz",
        "outputId": "8b818966-5d6d-48c2-ca5f-c233917e53a9"
      },
      "outputs": [],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict2000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_2000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict2000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeyaJaT9KW9X"
      },
      "source": [
        "We now train a model with a vocab of 3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWlEgWckKW9X",
        "outputId": "5e27cae4-d8de-4e0c-bd56-53b27476c912"
      },
      "outputs": [],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict3000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_3000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict3000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lqm338KKW9X"
      },
      "source": [
        "We now train a model with a vocab of 4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXwR3iooKW9X",
        "outputId": "d591dbee-4f37-4f8d-c446-ee327541438e"
      },
      "outputs": [],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict4000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_4000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict4000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee_ycyjbpQrx"
      },
      "source": [
        "## III Testing\n",
        "\n",
        "We will need a few functions. One for \"pasting\" the BPEs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4QxQWS-qNcC"
      },
      "outputs": [],
      "source": [
        "def decode_sp(list_sents):\n",
        "    return [''.join(sent).replace(' ', '').replace('▁', ' ').strip() for sent in list_sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L30TvaJzqPDP"
      },
      "source": [
        "One to extract the hypothesis in the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqXpCmX2pjFE"
      },
      "outputs": [],
      "source": [
        "def extract_hypothesis(filename):\n",
        "    outputs = []\n",
        "    with open(filename) as fp:\n",
        "        for line in fp:\n",
        "            # seulement les lignes qui commencet par H- (pour Hypothèse)\n",
        "            if 'H-' in line:\n",
        "                # prendre la 3ème colonne (c'est-à-dire l'indice 2)\n",
        "                outputs.append(line.strip().split('\\t')[2])\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya-Kvv_cKW9d"
      },
      "source": [
        "### III.a 1000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhNHQaCyKW9d"
      },
      "outputs": [],
      "source": [
        "import sentencepiece, os\n",
        "!mkdir -p $DIRECTORY/dev\n",
        "\n",
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_1000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict1000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_1000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yD-OOqpKW9d"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dOmZRSIKW9d",
        "outputId": "c9218563-2ad1-433b-f213-ee408d0f4021"
      },
      "outputs": [],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzMZcUvaKW9e"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwB2arlTKW9e",
        "outputId": "f9468a49-9565-441e-d818-5fd443bf6a15"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMgwM1UPKW9e"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDAF92GqKW9e"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z_cipI4KW9e"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQywcDxMKW9e"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR-DObUsKW9e"
      },
      "source": [
        "Word accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUhPBeHrKW9e"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK2KaeVEKW9e"
      },
      "source": [
        "### III.a 2000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3IVBrg_KW9e"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $DIRECTORY/dev\n",
        "\n",
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_2000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict2000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_2000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfo82HEyKW9e"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7gr2wVUKW9e"
      },
      "outputs": [],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNJgdP_UKW9e"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hFN26FyKW9f"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Xbju3LKW9f"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoCmtNnGKW9f"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYgictVjKW9f"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIygr2ANKW9f"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj6lCWULKW9f"
      },
      "source": [
        "Word accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgbAo2DrKW9f"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6tZ5euJKW9f"
      },
      "source": [
        "### III.b 3000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmkJ_u8nqjpY"
      },
      "outputs": [],
      "source": [
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_3000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict3000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_3000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbuUYD7aKW9f"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrK73j7isbrs"
      },
      "outputs": [],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4aNn3ZNqjMV"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENN0ebiIrU17"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "#BLEU\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4C0Cd3NKW9f"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBTqay7rKW9f"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VRQLXeWKW9f"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6jdGawvKW9g"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVJ0P2YqKW9g"
      },
      "source": [
        "Word accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DclTFxjrhqY"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8KaNVGXKW9g"
      },
      "source": [
        "### III.b 4000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFeQ9viPKW9g"
      },
      "outputs": [],
      "source": [
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_4000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict4000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_4000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GDE2DeHKW9g"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz0XgzDOKW9g"
      },
      "outputs": [],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F86u5uJxKW9g"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd-TbuyOKW9g"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "#BLEU\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txJqQJO9KW9g"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au2tPqOKKW9g"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL8hlhuIKW9g"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SBrJ0w5KW9g"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk5aG1uyKW9g"
      },
      "source": [
        "Word accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ3LtYOxKW9g"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "conda-norm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
