{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soniasol/test_normalisation_2/blob/main/notebooks/Train_norm_model_Sonia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIVO_5Oq3500"
      },
      "source": [
        "# Train a normaliser for French\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gabays/32M7131/blob/main/Cours_04/Cours04.ipynb)\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licence Creative Commons\" style=\"border-width:0;float:right;\\\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a>\n",
        "\n",
        "Simon Gabay (UniGE), Rachel Bawden (INRIA Paris)\n",
        "\n",
        "<img width=\"30px\" style=\"float:left\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"/>  Specific requirements for Colab users are signaled with this sign."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sonia's note**: If working with HPC, create a virtual environment and activate it."
      ],
      "metadata": {
        "id": "epurIP9mKdN5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS9in_4_LZ97"
      },
      "source": [
        "## I. Preparing the experiment\n",
        "\n",
        "You might need first to clean your env (OOD user in Geneva)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXFjCEFMKW9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b14f269f-f38a-4381-a44b-c2ce1a5b0318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: You must give at least one requirement to uninstall (see \"pip help uninstall\")\u001b[0m\u001b[31m\n",
            "\u001b[0mPackage                            Version\n",
            "---------------------------------- -------------------\n",
            "absl-py                            1.4.0\n",
            "accelerate                         1.3.0\n",
            "aiohappyeyeballs                   2.4.6\n",
            "aiohttp                            3.11.12\n",
            "aiosignal                          1.3.2\n",
            "alabaster                          1.0.0\n",
            "albucore                           0.0.23\n",
            "albumentations                     2.0.4\n",
            "ale-py                             0.10.2\n",
            "altair                             5.5.0\n",
            "annotated-types                    0.7.0\n",
            "anyio                              3.7.1\n",
            "argon2-cffi                        23.1.0\n",
            "argon2-cffi-bindings               21.2.0\n",
            "array_record                       0.6.0\n",
            "arviz                              0.20.0\n",
            "astropy                            7.0.1\n",
            "astropy-iers-data                  0.2025.2.17.0.34.13\n",
            "astunparse                         1.6.3\n",
            "atpublic                           4.1.0\n",
            "attrs                              25.1.0\n",
            "audioread                          3.0.1\n",
            "autograd                           1.7.0\n",
            "babel                              2.17.0\n",
            "backcall                           0.2.0\n",
            "beautifulsoup4                     4.13.3\n",
            "betterproto                        2.0.0b6\n",
            "bigframes                          1.37.0\n",
            "bigquery-magics                    0.6.0\n",
            "bleach                             6.2.0\n",
            "blinker                            1.9.0\n",
            "blis                               0.7.11\n",
            "blosc2                             3.1.1\n",
            "bokeh                              3.6.3\n",
            "Bottleneck                         1.4.2\n",
            "bqplot                             0.12.44\n",
            "branca                             0.8.1\n",
            "CacheControl                       0.14.2\n",
            "cachetools                         5.5.1\n",
            "catalogue                          2.0.10\n",
            "certifi                            2025.1.31\n",
            "cffi                               1.17.1\n",
            "chardet                            5.2.0\n",
            "charset-normalizer                 3.4.1\n",
            "chex                               0.1.88\n",
            "clarabel                           0.10.0\n",
            "click                              8.1.8\n",
            "cloudpathlib                       0.20.0\n",
            "cloudpickle                        3.1.1\n",
            "cmake                              3.31.4\n",
            "cmdstanpy                          1.2.5\n",
            "colorcet                           3.1.0\n",
            "colorlover                         0.3.0\n",
            "colour                             0.1.5\n",
            "community                          1.0.0b1\n",
            "confection                         0.1.5\n",
            "cons                               0.4.6\n",
            "contourpy                          1.3.1\n",
            "cramjam                            2.9.1\n",
            "cryptography                       43.0.3\n",
            "cuda-python                        12.6.0\n",
            "cudf-cu12                          24.12.0\n",
            "cufflinks                          0.17.3\n",
            "cupy-cuda12x                       13.3.0\n",
            "cvxopt                             1.3.2\n",
            "cvxpy                              1.6.0\n",
            "cycler                             0.12.1\n",
            "cyipopt                            1.5.0\n",
            "cymem                              2.0.11\n",
            "Cython                             3.0.12\n",
            "dask                               2024.10.0\n",
            "datascience                        0.17.6\n",
            "db-dtypes                          1.4.1\n",
            "dbus-python                        1.2.18\n",
            "debugpy                            1.8.0\n",
            "decorator                          4.4.2\n",
            "defusedxml                         0.7.1\n",
            "Deprecated                         1.2.18\n",
            "diffusers                          0.32.2\n",
            "distro                             1.9.0\n",
            "dlib                               19.24.2\n",
            "dm-tree                            0.1.9\n",
            "docker-pycreds                     0.4.0\n",
            "docstring_parser                   0.16\n",
            "docutils                           0.21.2\n",
            "dopamine_rl                        4.1.2\n",
            "duckdb                             1.1.3\n",
            "earthengine-api                    1.5.3\n",
            "easydict                           1.13\n",
            "editdistance                       0.8.1\n",
            "eerepr                             0.1.1\n",
            "einops                             0.8.1\n",
            "en-core-web-sm                     3.7.1\n",
            "entrypoints                        0.4\n",
            "et_xmlfile                         2.0.0\n",
            "etils                              1.12.0\n",
            "etuples                            0.3.9\n",
            "Farama-Notifications               0.0.4\n",
            "fastai                             2.7.18\n",
            "fastcore                           1.7.29\n",
            "fastdownload                       0.0.7\n",
            "fastjsonschema                     2.21.1\n",
            "fastprogress                       1.0.3\n",
            "fastrlock                          0.8.3\n",
            "filelock                           3.17.0\n",
            "firebase-admin                     6.6.0\n",
            "Flask                              3.1.0\n",
            "flatbuffers                        25.2.10\n",
            "flax                               0.10.3\n",
            "folium                             0.19.4\n",
            "fonttools                          4.56.0\n",
            "frozendict                         2.4.6\n",
            "frozenlist                         1.5.0\n",
            "fsspec                             2024.10.0\n",
            "future                             1.0.0\n",
            "gast                               0.6.0\n",
            "gcsfs                              2024.10.0\n",
            "GDAL                               3.6.4\n",
            "gdown                              5.2.0\n",
            "geemap                             0.35.1\n",
            "gensim                             4.3.3\n",
            "geocoder                           1.38.1\n",
            "geographiclib                      2.0\n",
            "geopandas                          1.0.1\n",
            "geopy                              2.4.1\n",
            "gin-config                         0.5.0\n",
            "gitdb                              4.0.12\n",
            "GitPython                          3.1.44\n",
            "glob2                              0.7\n",
            "google                             2.0.3\n",
            "google-ai-generativelanguage       0.6.15\n",
            "google-api-core                    2.24.1\n",
            "google-api-python-client           2.160.0\n",
            "google-auth                        2.27.0\n",
            "google-auth-httplib2               0.2.0\n",
            "google-auth-oauthlib               1.2.1\n",
            "google-cloud-aiplatform            1.79.0\n",
            "google-cloud-bigquery              3.29.0\n",
            "google-cloud-bigquery-connection   1.18.0\n",
            "google-cloud-bigquery-storage      2.28.0\n",
            "google-cloud-bigtable              2.28.1\n",
            "google-cloud-core                  2.4.1\n",
            "google-cloud-dataproc              5.17.0\n",
            "google-cloud-datastore             2.20.2\n",
            "google-cloud-firestore             2.20.0\n",
            "google-cloud-functions             1.19.0\n",
            "google-cloud-iam                   2.18.0\n",
            "google-cloud-language              2.16.0\n",
            "google-cloud-pubsub                2.25.0\n",
            "google-cloud-resource-manager      1.14.0\n",
            "google-cloud-spanner               3.51.0\n",
            "google-cloud-storage               2.19.0\n",
            "google-cloud-translate             3.19.0\n",
            "google-colab                       1.0.0\n",
            "google-crc32c                      1.6.0\n",
            "google-genai                       0.8.0\n",
            "google-generativeai                0.8.4\n",
            "google-pasta                       0.2.0\n",
            "google-resumable-media             2.7.2\n",
            "google-spark-connect               0.5.2\n",
            "googleapis-common-protos           1.67.0\n",
            "googledrivedownloader              1.1.0\n",
            "graphviz                           0.20.3\n",
            "greenlet                           3.1.1\n",
            "grpc-google-iam-v1                 0.14.0\n",
            "grpc-interceptor                   0.15.4\n",
            "grpcio                             1.70.0\n",
            "grpcio-status                      1.62.3\n",
            "grpclib                            0.4.7\n",
            "gspread                            6.1.4\n",
            "gspread-dataframe                  4.0.0\n",
            "gym                                0.25.2\n",
            "gym-notices                        0.0.8\n",
            "gymnasium                          1.0.0\n",
            "h11                                0.14.0\n",
            "h2                                 4.2.0\n",
            "h5netcdf                           1.5.0\n",
            "h5py                               3.12.1\n",
            "highspy                            1.9.0\n",
            "holidays                           0.67\n",
            "holoviews                          1.20.1\n",
            "hpack                              4.1.0\n",
            "html5lib                           1.1\n",
            "httpcore                           1.0.7\n",
            "httpimport                         1.4.0\n",
            "httplib2                           0.22.0\n",
            "httpx                              0.28.1\n",
            "huggingface-hub                    0.28.1\n",
            "humanize                           4.11.0\n",
            "hyperframe                         6.1.0\n",
            "hyperopt                           0.2.7\n",
            "ibis-framework                     9.2.0\n",
            "idna                               3.10\n",
            "imageio                            2.37.0\n",
            "imageio-ffmpeg                     0.6.0\n",
            "imagesize                          1.4.1\n",
            "imbalanced-learn                   0.13.0\n",
            "imgaug                             0.4.0\n",
            "immutabledict                      4.2.1\n",
            "importlib_metadata                 8.6.1\n",
            "importlib_resources                6.5.2\n",
            "imutils                            0.5.4\n",
            "inflect                            7.5.0\n",
            "iniconfig                          2.0.0\n",
            "intel-cmplr-lib-ur                 2025.0.4\n",
            "intel-openmp                       2025.0.4\n",
            "ipyevents                          2.0.2\n",
            "ipyfilechooser                     0.6.0\n",
            "ipykernel                          6.17.1\n",
            "ipyleaflet                         0.19.2\n",
            "ipyparallel                        8.8.0\n",
            "ipython                            7.34.0\n",
            "ipython-genutils                   0.2.0\n",
            "ipython-sql                        0.5.0\n",
            "ipytree                            0.2.2\n",
            "ipywidgets                         7.7.1\n",
            "itsdangerous                       2.2.0\n",
            "jax                                0.4.33\n",
            "jax-cuda12-pjrt                    0.4.33\n",
            "jax-cuda12-plugin                  0.4.33\n",
            "jaxlib                             0.4.33\n",
            "jeepney                            0.7.1\n",
            "jellyfish                          1.1.0\n",
            "jieba                              0.42.1\n",
            "Jinja2                             3.1.5\n",
            "jiter                              0.8.2\n",
            "joblib                             1.4.2\n",
            "jsonpatch                          1.33\n",
            "jsonpickle                         4.0.2\n",
            "jsonpointer                        3.0.0\n",
            "jsonschema                         4.23.0\n",
            "jsonschema-specifications          2024.10.1\n",
            "jupyter-client                     6.1.12\n",
            "jupyter-console                    6.1.0\n",
            "jupyter_core                       5.7.2\n",
            "jupyter-leaflet                    0.19.2\n",
            "jupyter-server                     1.24.0\n",
            "jupyterlab_pygments                0.3.0\n",
            "jupyterlab_widgets                 3.0.13\n",
            "kaggle                             1.6.17\n",
            "kagglehub                          0.3.9\n",
            "keras                              3.8.0\n",
            "keras-hub                          0.18.1\n",
            "keras-nlp                          0.18.1\n",
            "keyring                            23.5.0\n",
            "kiwisolver                         1.4.8\n",
            "langchain                          0.3.19\n",
            "langchain-core                     0.3.37\n",
            "langchain-text-splitters           0.3.6\n",
            "langcodes                          3.5.0\n",
            "langsmith                          0.3.8\n",
            "language_data                      1.3.0\n",
            "launchpadlib                       1.10.16\n",
            "lazr.restfulclient                 0.14.4\n",
            "lazr.uri                           1.0.6\n",
            "lazy_loader                        0.4\n",
            "libclang                           18.1.1\n",
            "libcudf-cu12                       24.12.0\n",
            "libkvikio-cu12                     24.12.1\n",
            "librosa                            0.10.2.post1\n",
            "lightgbm                           4.5.0\n",
            "linkify-it-py                      2.0.3\n",
            "llvmlite                           0.44.0\n",
            "locket                             1.0.0\n",
            "logical-unification                0.4.6\n",
            "lxml                               5.3.1\n",
            "marisa-trie                        1.2.1\n",
            "Markdown                           3.7\n",
            "markdown-it-py                     3.0.0\n",
            "MarkupSafe                         3.0.2\n",
            "matplotlib                         3.10.0\n",
            "matplotlib-inline                  0.1.7\n",
            "matplotlib-venn                    1.1.1\n",
            "mdit-py-plugins                    0.4.2\n",
            "mdurl                              0.1.2\n",
            "miniKanren                         1.0.3\n",
            "missingno                          0.5.2\n",
            "mistune                            3.1.2\n",
            "mizani                             0.13.1\n",
            "mkl                                2025.0.1\n",
            "ml-dtypes                          0.4.1\n",
            "mlxtend                            0.23.4\n",
            "more-itertools                     10.6.0\n",
            "moviepy                            1.0.3\n",
            "mpmath                             1.3.0\n",
            "msgpack                            1.1.0\n",
            "multidict                          6.1.0\n",
            "multipledispatch                   1.0.0\n",
            "multitasking                       0.0.11\n",
            "murmurhash                         1.0.12\n",
            "music21                            9.3.0\n",
            "namex                              0.0.8\n",
            "narwhals                           1.27.1\n",
            "natsort                            8.4.0\n",
            "nbclassic                          1.2.0\n",
            "nbclient                           0.10.2\n",
            "nbconvert                          7.16.6\n",
            "nbformat                           5.10.4\n",
            "ndindex                            1.9.2\n",
            "nest-asyncio                       1.6.0\n",
            "networkx                           3.4.2\n",
            "nibabel                            5.3.2\n",
            "nltk                               3.9.1\n",
            "notebook                           6.5.5\n",
            "notebook_shim                      0.2.4\n",
            "numba                              0.61.0\n",
            "numba-cuda                         0.0.17.1\n",
            "numexpr                            2.10.2\n",
            "numpy                              1.25.2\n",
            "nvidia-cublas-cu12                 12.5.3.2\n",
            "nvidia-cuda-cupti-cu12             12.5.82\n",
            "nvidia-cuda-nvcc-cu12              12.5.82\n",
            "nvidia-cuda-nvrtc-cu12             12.5.82\n",
            "nvidia-cuda-runtime-cu12           12.5.82\n",
            "nvidia-cudnn-cu12                  9.3.0.75\n",
            "nvidia-cufft-cu12                  11.2.3.61\n",
            "nvidia-curand-cu12                 10.3.6.82\n",
            "nvidia-cusolver-cu12               11.6.3.83\n",
            "nvidia-cusparse-cu12               12.5.1.3\n",
            "nvidia-nccl-cu12                   2.21.5\n",
            "nvidia-nvcomp-cu12                 4.1.0.6\n",
            "nvidia-nvjitlink-cu12              12.5.82\n",
            "nvidia-nvtx-cu12                   12.4.127\n",
            "nvtx                               0.2.10\n",
            "nx-cugraph-cu12                    24.12.0\n",
            "oauth2client                       4.1.3\n",
            "oauthlib                           3.2.2\n",
            "openai                             1.61.1\n",
            "opencv-contrib-python              4.11.0.86\n",
            "opencv-python                      4.11.0.86\n",
            "opencv-python-headless             4.11.0.86\n",
            "openpyxl                           3.1.5\n",
            "opentelemetry-api                  1.16.0\n",
            "opentelemetry-sdk                  1.16.0\n",
            "opentelemetry-semantic-conventions 0.37b0\n",
            "opt_einsum                         3.4.0\n",
            "optax                              0.2.4\n",
            "optree                             0.14.0\n",
            "orbax-checkpoint                   0.6.4\n",
            "orjson                             3.10.15\n",
            "osqp                               0.6.7.post3\n",
            "packaging                          24.2\n",
            "pandas                             2.2.2\n",
            "pandas-datareader                  0.10.0\n",
            "pandas-gbq                         0.27.0\n",
            "pandas-stubs                       2.2.2.240909\n",
            "pandocfilters                      1.5.1\n",
            "panel                              1.6.1\n",
            "param                              2.2.0\n",
            "parso                              0.8.4\n",
            "parsy                              2.1\n",
            "partd                              1.4.2\n",
            "pathlib                            1.0.1\n",
            "patsy                              1.0.1\n",
            "peewee                             3.17.9\n",
            "peft                               0.14.0\n",
            "pexpect                            4.9.0\n",
            "pickleshare                        0.7.5\n",
            "pillow                             11.1.0\n",
            "pip                                24.1.2\n",
            "platformdirs                       4.3.6\n",
            "plotly                             5.24.1\n",
            "plotnine                           0.14.5\n",
            "pluggy                             1.5.0\n",
            "ply                                3.11\n",
            "polars                             1.9.0\n",
            "pooch                              1.8.2\n",
            "portpicker                         1.5.2\n",
            "preshed                            3.0.9\n",
            "prettytable                        3.14.0\n",
            "proglog                            0.1.10\n",
            "progressbar2                       4.5.0\n",
            "prometheus_client                  0.21.1\n",
            "promise                            2.3\n",
            "prompt_toolkit                     3.0.50\n",
            "propcache                          0.2.1\n",
            "prophet                            1.1.6\n",
            "proto-plus                         1.26.0\n",
            "protobuf                           4.25.6\n",
            "psutil                             5.9.5\n",
            "psycopg2                           2.9.10\n",
            "ptyprocess                         0.7.0\n",
            "py-cpuinfo                         9.0.0\n",
            "py4j                               0.10.9.7\n",
            "pyarrow                            17.0.0\n",
            "pyasn1                             0.6.1\n",
            "pyasn1_modules                     0.4.1\n",
            "pycocotools                        2.0.8\n",
            "pycparser                          2.22\n",
            "pydantic                           2.10.6\n",
            "pydantic_core                      2.27.2\n",
            "pydata-google-auth                 1.9.1\n",
            "pydot                              3.0.4\n",
            "pydotplus                          2.0.2\n",
            "PyDrive                            1.3.1\n",
            "PyDrive2                           1.21.3\n",
            "pyerfa                             2.0.1.5\n",
            "pygame                             2.6.1\n",
            "pygit2                             1.17.0\n",
            "Pygments                           2.18.0\n",
            "PyGObject                          3.42.1\n",
            "PyJWT                              2.10.1\n",
            "pylibcudf-cu12                     24.12.0\n",
            "pylibcugraph-cu12                  24.12.0\n",
            "pylibraft-cu12                     24.12.0\n",
            "pymc                               5.20.1\n",
            "pymystem3                          0.2.0\n",
            "pynvjitlink-cu12                   0.5.0\n",
            "pyogrio                            0.10.0\n",
            "Pyomo                              6.8.2\n",
            "PyOpenGL                           3.1.9\n",
            "pyOpenSSL                          24.2.1\n",
            "pyparsing                          3.2.1\n",
            "pyperclip                          1.9.0\n",
            "pyproj                             3.7.1\n",
            "pyshp                              2.3.1\n",
            "PySocks                            1.7.1\n",
            "pyspark                            3.5.4\n",
            "pytensor                           2.27.1\n",
            "pytest                             8.3.4\n",
            "python-apt                         0.0.0\n",
            "python-box                         7.3.2\n",
            "python-dateutil                    2.8.2\n",
            "python-louvain                     0.16\n",
            "python-slugify                     8.0.4\n",
            "python-snappy                      0.7.3\n",
            "python-utils                       3.9.1\n",
            "pytz                               2025.1\n",
            "pyviz_comms                        3.0.4\n",
            "PyYAML                             6.0.2\n",
            "pyzmq                              24.0.1\n",
            "qdldl                              0.1.7.post5\n",
            "ratelim                            0.1.6\n",
            "referencing                        0.36.2\n",
            "regex                              2024.11.6\n",
            "requests                           2.32.3\n",
            "requests-oauthlib                  2.0.0\n",
            "requests-toolbelt                  1.0.0\n",
            "requirements-parser                0.9.0\n",
            "rich                               13.9.4\n",
            "rmm-cu12                           24.12.1\n",
            "rpds-py                            0.22.3\n",
            "rpy2                               3.4.2\n",
            "rsa                                4.9\n",
            "safetensors                        0.5.2\n",
            "scikit-image                       0.25.2\n",
            "scikit-learn                       1.6.1\n",
            "scipy                              1.13.1\n",
            "scooby                             0.10.0\n",
            "scs                                3.2.7.post2\n",
            "seaborn                            0.13.2\n",
            "SecretStorage                      3.3.1\n",
            "Send2Trash                         1.8.3\n",
            "sentence-transformers              3.4.1\n",
            "sentencepiece                      0.2.0\n",
            "sentry-sdk                         2.22.0\n",
            "setproctitle                       1.3.4\n",
            "setuptools                         75.1.0\n",
            "shap                               0.46.0\n",
            "shapely                            2.0.7\n",
            "shellingham                        1.5.4\n",
            "simple-parsing                     0.1.7\n",
            "simsimd                            6.2.1\n",
            "six                                1.17.0\n",
            "sklearn-compat                     0.1.3\n",
            "sklearn-pandas                     2.2.0\n",
            "slicer                             0.0.8\n",
            "smart-open                         7.1.0\n",
            "smmap                              5.0.2\n",
            "sniffio                            1.3.1\n",
            "snowballstemmer                    2.2.0\n",
            "soundfile                          0.13.1\n",
            "soupsieve                          2.6\n",
            "soxr                               0.5.0.post1\n",
            "spacy                              3.7.5\n",
            "spacy-legacy                       3.0.12\n",
            "spacy-loggers                      1.0.5\n",
            "spanner-graph-notebook             1.1.1\n",
            "Sphinx                             8.1.3\n",
            "sphinxcontrib-applehelp            2.0.0\n",
            "sphinxcontrib-devhelp              2.0.0\n",
            "sphinxcontrib-htmlhelp             2.1.0\n",
            "sphinxcontrib-jsmath               1.0.1\n",
            "sphinxcontrib-qthelp               2.0.0\n",
            "sphinxcontrib-serializinghtml      2.0.0\n",
            "SQLAlchemy                         2.0.38\n",
            "sqlglot                            25.6.1\n",
            "sqlparse                           0.5.3\n",
            "srsly                              2.5.1\n",
            "stanio                             0.5.1\n",
            "statsmodels                        0.14.4\n",
            "stringzilla                        3.11.3\n",
            "sympy                              1.13.1\n",
            "tables                             3.10.2\n",
            "tabulate                           0.9.0\n",
            "tbb                                2022.0.0\n",
            "tcmlib                             1.2.0\n",
            "tenacity                           9.0.0\n",
            "tensorboard                        2.18.0\n",
            "tensorboard-data-server            0.7.2\n",
            "tensorflow                         2.18.0\n",
            "tensorflow-datasets                4.9.7\n",
            "tensorflow-hub                     0.16.1\n",
            "tensorflow-io-gcs-filesystem       0.37.1\n",
            "tensorflow-metadata                1.16.1\n",
            "tensorflow-probability             0.25.0\n",
            "tensorflow-text                    2.18.1\n",
            "tensorstore                        0.1.71\n",
            "termcolor                          2.5.0\n",
            "terminado                          0.18.1\n",
            "text-unidecode                     1.3\n",
            "textblob                           0.19.0\n",
            "tf_keras                           2.18.0\n",
            "tf-slim                            1.1.0\n",
            "thinc                              8.2.5\n",
            "threadpoolctl                      3.5.0\n",
            "tifffile                           2025.2.18\n",
            "timm                               1.0.14\n",
            "tinycss2                           1.4.0\n",
            "tokenizers                         0.21.0\n",
            "toml                               0.10.2\n",
            "toolz                              0.12.1\n",
            "torch                              2.5.1+cu124\n",
            "torchaudio                         2.5.1+cu124\n",
            "torchsummary                       1.5.1\n",
            "torchvision                        0.20.1+cu124\n",
            "tornado                            6.4.2\n",
            "tqdm                               4.67.1\n",
            "traitlets                          5.7.1\n",
            "traittypes                         0.2.1\n",
            "transformers                       4.48.3\n",
            "treescope                          0.1.9\n",
            "triton                             3.1.0\n",
            "tweepy                             4.15.0\n",
            "typeguard                          4.4.2\n",
            "typer                              0.15.1\n",
            "types-pytz                         2025.1.0.20250204\n",
            "types-setuptools                   75.8.0.20250210\n",
            "typing_extensions                  4.12.2\n",
            "tzdata                             2025.1\n",
            "tzlocal                            5.3\n",
            "uc-micro-py                        1.0.3\n",
            "umf                                0.9.1\n",
            "uritemplate                        4.1.1\n",
            "urllib3                            2.3.0\n",
            "vega-datasets                      0.9.0\n",
            "wadllib                            1.3.6\n",
            "wandb                              0.19.6\n",
            "wasabi                             1.1.3\n",
            "wcwidth                            0.2.13\n",
            "weasel                             0.4.1\n",
            "webcolors                          24.11.1\n",
            "webencodings                       0.5.1\n",
            "websocket-client                   1.8.0\n",
            "websockets                         14.2\n",
            "Werkzeug                           3.1.3\n",
            "wheel                              0.45.1\n",
            "widgetsnbextension                 3.6.10\n",
            "wordcloud                          1.9.4\n",
            "wrapt                              1.17.2\n",
            "xarray                             2025.1.2\n",
            "xarray-einstats                    0.8.0\n",
            "xgboost                            2.1.4\n",
            "xlrd                               2.0.1\n",
            "xyzservices                        2025.1.0\n",
            "yarl                               1.18.3\n",
            "yellowbrick                        1.5\n",
            "yfinance                           0.2.54\n",
            "zipp                               3.21.0\n",
            "zstandard                          0.23.0\n"
          ]
        }
      ],
      "source": [
        "!pip freeze --user --exclude-editable | xargs pip uninstall -y\n",
        "!pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sonia's note**: about the above command\n",
        "\n",
        "1. !pip freeze --user --exclude-editable: Lists all installed Python packages for the current user, excluding editable packages.\n",
        "2. | xargs pip uninstall -y: Uninstalls all those packages without requiring confirmation (-y flag).\n",
        "3. !pip list: Lists the remaining installed packages after uninstallation.\n",
        "\n",
        "This is useful for cleaning up user-installed packages before reinstalling only necessary dependencies."
      ],
      "metadata": {
        "id": "fFm2yi8hJwPO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD2irpZSKW9U"
      },
      "source": [
        "Installing required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG3o78iI30sR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c0735e6-57d0-4959-d816-bb69915aa4ed",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq==0.12.2\n",
            "  Using cached fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting sacrebleu\n",
            "  Using cached sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "Collecting omegaconf==2.0.5\n",
            "  Using cached omegaconf-2.0.5-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.5 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf==2.0.5 from https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following yanked versions: 1.0.0, 1.0.1, 1.0.2, 2.0.0rc1, 2.0.0rc2, 2.0.0rc22, 2.0.0rc23, 2.0.0rc24, 2.0.0rc25, 2.0.0rc26, 2.0.0rc27, 2.0.0rc28, 2.0.0rc29, 2.0.1rc1, 2.0.1rc2, 2.0.1rc3, 2.0.1rc4, 2.0.1rc5, 2.2.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement omegaconf==2.0.5 (from versions: 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.0.8, 1.0.9, 1.0.10, 1.0.11, 1.0.12, 1.0.13, 1.0.14, 1.0.16, 1.0.17, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.1.8, 1.1.9, 1.1.10, 1.2.0, 1.2.1, 1.3.0rc1, 1.3.0rc2, 1.3.0rc3, 1.3.0rc4, 1.3.0rc5, 1.3.0rc6, 1.3.0rc7, 1.3.0rc8, 1.3.0rc9, 1.3.0rc10, 1.3.0, 1.4.0rc1, 1.4.0rc2, 1.4.0rc3, 1.4.0rc4, 1.4.0, 1.4.1rc1, 1.4.1, 2.0.0rc3, 2.0.0rc4, 2.0.0rc5, 2.0.0rc6, 2.0.0rc7, 2.0.0rc8, 2.0.0rc9, 2.0.0rc10, 2.0.0rc11, 2.0.0rc12, 2.0.0rc13, 2.0.0rc14, 2.0.0rc15, 2.0.0rc16, 2.0.0rc17, 2.0.0rc18, 2.0.0rc19, 2.0.0rc20, 2.0.0rc21, 2.0.0, 2.0.1rc6, 2.0.1rc7, 2.0.1rc8, 2.0.1rc9, 2.0.1rc10, 2.0.1rc11, 2.0.1rc12, 2.0.1rc13, 2.0.1, 2.0.2rc1, 2.0.2rc2, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.1.0.dev1, 2.1.0.dev2, 2.1.0.dev3, 2.1.0.dev4, 2.1.0.dev5, 2.1.0.dev6, 2.1.0.dev7, 2.1.0.dev8, 2.1.0.dev9, 2.1.0.dev10, 2.1.0.dev11, 2.1.0.dev12, 2.1.0.dev13, 2.1.0.dev14, 2.1.0.dev15, 2.1.0.dev16, 2.1.0.dev17, 2.1.0.dev18, 2.1.0.dev19, 2.1.0.dev20, 2.1.0.dev21, 2.1.0.dev22, 2.1.0.dev23, 2.1.0.dev24, 2.1.0.dev25, 2.1.0.dev26, 2.1.0.dev27, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.2.0.dev1, 2.2.0.dev2, 2.2.0.dev3, 2.2.0.dev4, 2.2.0.dev5, 2.2.1, 2.2.2, 2.2.3, 2.3.0.dev0, 2.3.0.dev1, 2.3.0.dev2, 2.3.0, 2.4.0.dev0, 2.4.0.dev1, 2.4.0.dev2, 2.4.0.dev3)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for omegaconf==2.0.5\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install fairseq==0.12.2 sentencepiece sacrebleu omegaconf==2.0.5 gdown==4.2.0 tensorboardX numpy==1.25.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sonia's note**: about the above command\n",
        "\n",
        "We install the following Python packages with specific versions:\n",
        "\n",
        "1. fairseq==0.12.2: A sequence modeling toolkit for training custom neural networks.\n",
        "2. sentencepiece: A subword tokenization library.\n",
        "3. sacrebleu: A library for BLEU score calculation in machine translation.\n",
        "4. omegaconf==2.0.5: A hierarchical configuration library for Python.\n",
        "5. gdown==4.2.0: A utility for downloading files from Google Drive.\n",
        "6. tensorboardX: A TensorBoard logger for PyTorch.\n",
        "7. numpy==1.25.2: A fundamental numerical computing library.\n",
        "\n",
        "This ensures the correct dependencies for working with fairseq."
      ],
      "metadata": {
        "id": "SrlM-xQpK349"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meKj3h29KW9U"
      },
      "source": [
        "### I.a Colab vs other tools\n",
        "\n",
        "<img width=\"30px\" style=\"float:left\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"/>  Colab users:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-cheDZ4KW9U"
      },
      "outputs": [],
      "source": [
        "DIRECTORY=\"/content/FreEMnorm\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sonia's note**: about the above command\n",
        "\n",
        "We set an environment variable DIRECTORY with the path \"/content/FreEMnorm\". This is commonly used to define a working directory."
      ],
      "metadata": {
        "id": "aWyiyh0HLrQN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TCc3zHOKW9U"
      },
      "source": [
        "Non colab users:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5_HRJ3OKW9U"
      },
      "outputs": [],
      "source": [
        "DIRECTORY=\"FreEMnorm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZoNmqSlKhSu"
      },
      "source": [
        "### I.b Retrieving data\n",
        "We download the repo (`dev` branch only for now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "79e9t2dvJWZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9c678f-f443-431d-8e60-1623d593515e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FreEMnorm'...\n",
            "remote: Enumerating objects: 391, done.\u001b[K\n",
            "remote: Counting objects: 100% (391/391), done.\u001b[K\n",
            "remote: Compressing objects: 100% (251/251), done.\u001b[K\n",
            "remote: Total 391 (delta 205), reused 323 (delta 137), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (391/391), 3.14 MiB | 11.93 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n"
          ]
        }
      ],
      "source": [
        "#delete if there is an older version of the repo\n",
        "!rm -rf $DIRECTORY\n",
        "# cloning dev branch\n",
        "!git clone -b dev https://github.com/FreEM-corpora/FreEMnorm.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sonia's note**: about the above command\n",
        "\n",
        "We use the -b dev option to clone the development (dev) branch instead of the default main or master."
      ],
      "metadata": {
        "id": "fK4_u83wMhE-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gHRyxOoKted"
      },
      "source": [
        "### I.c If you want to redo the split and the creation of the training data\n",
        "\n",
        "We can split the corpus and create a file with the data in the requires format.\n",
        "\n",
        "<img width=\"30px\" style=\"float:left\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"/> Add `-c` to the last command (`split_to_src_trg.py`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzRsHEzd4WU3"
      },
      "outputs": [],
      "source": [
        "#Remove previously split directory if there is any\n",
        "!rm -rf $DIRECTORY/split/ $DIRECTORY/data/\n",
        "!echo \"Split and Data directories have been removed\"\n",
        "#We split the corpus into train/test/dev\n",
        "!python $DIRECTORY/split.py\n",
        "#We turn the split into the requires format\n",
        "!python $DIRECTORY/split_to_src_trg.py\n",
        "#Colab users should comment the previous one and use the following\n",
        "#!python /content/FreEMnorm/split_to_src_trg.py -c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLr0uJSPLmd7"
      },
      "source": [
        "## II. Traing a model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeQzsF3Lo9Qt"
      },
      "source": [
        "### II.a Preprocessing\n",
        "\n",
        "We will need a few functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H1PQJScKW9V"
      },
      "outputs": [],
      "source": [
        "# Read a file per line\n",
        "def read_file(filename):\n",
        "  list_sents = []\n",
        "  with open(filename) as fp:\n",
        "    for line in fp:\n",
        "      list_sents.append(line.strip())\n",
        "  return list_sents\n",
        "\n",
        "#write a file per line\n",
        "def write_file(list_sents, filename):\n",
        "    with open(filename, 'w') as fp:\n",
        "        for sent in list_sents:\n",
        "            fp.write(sent + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUDgRxGVKW9V"
      },
      "source": [
        "We will need various sizes of vocabulary: 2000, 3000, 4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIMXi--YLuHG"
      },
      "outputs": [],
      "source": [
        "import sentencepiece\n",
        "import os\n",
        "\n",
        "!rm -rf $DIRECTORY/data/vocabulary.src-trg\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_1000\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_2000\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_3000\n",
        "!rm -rf $DIRECTORY/data/data_norm_bin_4000\n",
        "\n",
        "# We make a big file with all the data\n",
        "!cat $DIRECTORY/data/* > $DIRECTORY/data/vocabulary.src-trg\n",
        "\n",
        "# 1000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_1000\"),\n",
        "                               vocab_size=1000)\n",
        "\n",
        "# 2000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_2000\"),\n",
        "                               vocab_size=2000)\n",
        "\n",
        "#3000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_3000\"),\n",
        "                               vocab_size=3000)\n",
        "\n",
        "#4000\n",
        "sentencepiece.SentencePieceTrainer.train(input=os.path.join(DIRECTORY,\"data/vocabulary.src-trg\"),\n",
        "                               model_prefix=os.path.join(DIRECTORY,\"data/bpe_joint_4000\"),\n",
        "                               vocab_size=4000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYweZ4Cmfet1"
      },
      "source": [
        "We prepare the various datasets with 1000 wods vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AnAUlyuKW9V",
        "outputId": "ba0fab5f-2c95-4e24-b2a6-59d5200742e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19483 19483\n",
            "2617 2617\n",
            "5879 5879\n"
          ]
        }
      ],
      "source": [
        "#Loading datasets\n",
        "train_src = read_file(os.path.join(DIRECTORY,'data/train.src'))\n",
        "train_trg = read_file(os.path.join(DIRECTORY,'data/train.trg'))\n",
        "dev_src = read_file(os.path.join(DIRECTORY,'data/dev.src'))\n",
        "dev_trg = read_file(os.path.join(DIRECTORY,'data/dev.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "\n",
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_1000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp1000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp1000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp1000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp1000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp1000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp1000.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpu6eFkJKW9W"
      },
      "source": [
        "2000 words vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q4vTqi6fiCl",
        "outputId": "ed32c510-8884-41b5-bb25-e023ab25a9af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19483 19483\n",
            "2617 2617\n",
            "5879 5879\n"
          ]
        }
      ],
      "source": [
        "#Loading datasets\n",
        "train_src = read_file(os.path.join(DIRECTORY,'data/train.src'))\n",
        "train_trg = read_file(os.path.join(DIRECTORY,'data/train.trg'))\n",
        "dev_src = read_file(os.path.join(DIRECTORY,'data/dev.src'))\n",
        "dev_trg = read_file(os.path.join(DIRECTORY,'data/dev.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "\n",
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_2000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp2000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp2000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp2000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp2000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp2000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp2000.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtjAPohKW9W"
      },
      "source": [
        "3000 words vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ML_XId-KW9W",
        "outputId": "c8c8c7b0-0a52-4397-e0c8-55eabd31d574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19483 19483\n",
            "2617 2617\n",
            "5879 5879\n"
          ]
        }
      ],
      "source": [
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_3000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp3000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp3000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp3000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp3000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp3000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp3000.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QThTgqDKW9W"
      },
      "source": [
        "4000 words vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XupMx_7SKW9W",
        "outputId": "387a20b4-4d34-41ad-b27b-f18b6d64b1bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19483 19483\n",
            "2617 2617\n",
            "5879 5879\n"
          ]
        }
      ],
      "source": [
        "# Loading the bpe model\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_4000.model'))\n",
        "\n",
        "# Apply the bpe model to the datasets\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# Checking the result (src and trg should have the same length)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# We create the files bpe-zed\n",
        "write_file([' '.join(sent) for sent in train_src_sp], os.path.join(DIRECTORY,'data/train.sp4000.src'))\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], os.path.join(DIRECTORY,'data/train.sp4000.trg'))\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], os.path.join(DIRECTORY,'data/dev.sp4000.src'))\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], os.path.join(DIRECTORY,'data/dev.sp4000.trg'))\n",
        "write_file([' '.join(sent) for sent in test_src_sp], os.path.join(DIRECTORY,'data/test.sp4000.src'))\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], os.path.join(DIRECTORY,'data/test.sp4000.trg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngPU1B3wg4hG",
        "outputId": "0ab4cc9d-4976-44fe-97e7-566ef72d12d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: fairseq-preprocess: command not found\n",
            "/bin/bash: line 1: fairseq-preprocess: command not found\n",
            "/bin/bash: line 1: fairseq-preprocess: command not found\n",
            "/bin/bash: line 1: fairseq-preprocess: command not found\n"
          ]
        }
      ],
      "source": [
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_1000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp1000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp1000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp1000 \\\n",
        "                    --joined-dictionary\n",
        "\n",
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_2000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp2000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp2000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp2000 \\\n",
        "                    --joined-dictionary\n",
        "\n",
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_3000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp3000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp3000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp3000 \\\n",
        "                    --joined-dictionary\n",
        "\n",
        "!fairseq-preprocess --destdir $DIRECTORY/data/data_norm_bin_4000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref $DIRECTORY/data/train.sp4000 \\\n",
        "                    --validpref $DIRECTORY/data/dev.sp4000 \\\n",
        "                    --testpref $DIRECTORY/data/test.sp4000 \\\n",
        "                    --joined-dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yqxh3vSnZSq"
      },
      "source": [
        "### II.b Training\n",
        "\n",
        "We now train a model with a vocab of 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5X-AMlzKW9X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "outputId": "5651676c-2cec-405d-e9c2-cfd8ab00782d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.25.2\n",
            "Uninstalling numpy-1.25.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/f2py3\n",
            "    /usr/local/bin/f2py3.11\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy-1.25.2.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libopenblas64_p-r0-5007b62f.3.23.dev.so\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled numpy-1.25.2\n",
            "Collecting numpy==1.25.2\n",
            "  Using cached numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "508b56a09e1342278a16bfde81264ea8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpyy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-54638c5861c7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip uninstall numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install numpy==1.25.2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpyy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpyy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!pip uninstall numpy\n",
        "!pip install numpy==1.25.2\n",
        "import numpyy\n",
        "numpy.version.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T583f25KW9X",
        "outputId": "62cbac37-e7bd-4cc3-c694-58569d75d7f9",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: fairseq-train: command not found\n"
          ]
        }
      ],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict1000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_1000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict1000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65_zng4wKW9X"
      },
      "source": [
        "We now train a model with a vocab of 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRuUuPBlhxQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b818966-5d6d-48c2-ca5f-c233917e53a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: fairseq-train: command not found\n"
          ]
        }
      ],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict2000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_2000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict2000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeyaJaT9KW9X"
      },
      "source": [
        "We now train a model with a vocab of 3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWlEgWckKW9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e27cae4-d8de-4e0c-bd56-53b27476c912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: fairseq-train: command not found\n"
          ]
        }
      ],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict3000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_3000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict3000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lqm338KKW9X"
      },
      "source": [
        "We now train a model with a vocab of 4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXwR3iooKW9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d591dbee-4f37-4f8d-c446-ee327541438e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: fairseq-train: command not found\n"
          ]
        }
      ],
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir -p $DIRECTORY/models/lstm_dict4000_3l_embed384\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "       $DIRECTORY/data/data_norm_bin_4000 \\\n",
        "        --save-dir $DIRECTORY/models/lstm_dict4000_3l_embed384 \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee_ycyjbpQrx"
      },
      "source": [
        "## III Testing\n",
        "\n",
        "We will need a few functions. One for \"pasting\" the BPEs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4QxQWS-qNcC"
      },
      "outputs": [],
      "source": [
        "def decode_sp(list_sents):\n",
        "    return [''.join(sent).replace(' ', '').replace('▁', ' ').strip() for sent in list_sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L30TvaJzqPDP"
      },
      "source": [
        "One to extract the hypothesis in the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqXpCmX2pjFE"
      },
      "outputs": [],
      "source": [
        "def extract_hypothesis(filename):\n",
        "    outputs = []\n",
        "    with open(filename) as fp:\n",
        "        for line in fp:\n",
        "            # seulement les lignes qui commencet par H- (pour Hypothèse)\n",
        "            if 'H-' in line:\n",
        "                # prendre la 3ème colonne (c'est-à-dire l'indice 2)\n",
        "                outputs.append(line.strip().split('\\t')[2])\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya-Kvv_cKW9d"
      },
      "source": [
        "### III.a 1000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhNHQaCyKW9d"
      },
      "outputs": [],
      "source": [
        "import sentencepiece, os\n",
        "!mkdir -p $DIRECTORY/dev\n",
        "\n",
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_1000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict1000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_1000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yD-OOqpKW9d"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dOmZRSIKW9d",
        "outputId": "c9218563-2ad1-433b-f213-ee408d0f4021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-20 21:49:57 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'FreEMnorm/data/data_norm_bin_1000', 'source_lang': 'src', 'target_lang': 'trg', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-06-20 21:49:57 | INFO | fairseq.tasks.translation | [src] dictionary: 952 types\n",
            "2024-06-20 21:49:57 | INFO | fairseq.tasks.translation | [trg] dictionary: 952 types\n",
            "2024-06-20 21:49:57 | INFO | fairseq_cli.interactive | loading model(s) from FreEMnorm/models/lstm_dict1000_3l_embed384/checkpoint_best.pt\n",
            "2024-06-20 21:50:01 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-06-20 21:50:01 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2024-06-20 22:00:34 | INFO | fairseq_cli.interactive | Total time: 636.929 seconds; translation time: 591.704\n"
          ]
        }
      ],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzMZcUvaKW9e"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwB2arlTKW9e",
        "outputId": "f9468a49-9565-441e-d818-5fd443bf6a15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BLEU = 52.51 77.4/59.4/46.2/35.8 (BP = 1.000 ratio = 1.003 hyp_len = 82838 ref_len = 82598)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMgwM1UPKW9e"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDAF92GqKW9e"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z_cipI4KW9e"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQywcDxMKW9e"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR-DObUsKW9e"
      },
      "source": [
        "Word accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUhPBeHrKW9e"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK2KaeVEKW9e"
      },
      "source": [
        "### III.a 2000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3IVBrg_KW9e"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $DIRECTORY/dev\n",
        "\n",
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_2000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict2000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_2000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfo82HEyKW9e"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7gr2wVUKW9e"
      },
      "outputs": [],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNJgdP_UKW9e"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hFN26FyKW9f"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Xbju3LKW9f"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoCmtNnGKW9f"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYgictVjKW9f"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIygr2ANKW9f"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj6lCWULKW9f"
      },
      "source": [
        "Word accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgbAo2DrKW9f"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6tZ5euJKW9f"
      },
      "source": [
        "### III.b 3000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmkJ_u8nqjpY"
      },
      "outputs": [],
      "source": [
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_3000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict3000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_3000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbuUYD7aKW9f"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrK73j7isbrs"
      },
      "outputs": [],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4aNn3ZNqjMV"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENN0ebiIrU17"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "#BLEU\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4C0Cd3NKW9f"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBTqay7rKW9f"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VRQLXeWKW9f"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6jdGawvKW9g"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVJ0P2YqKW9g"
      },
      "source": [
        "Word accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DclTFxjrhqY"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8KaNVGXKW9g"
      },
      "source": [
        "### III.b 4000 words\n",
        "\n",
        "Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFeQ9viPKW9g"
      },
      "outputs": [],
      "source": [
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.sp.src.tmp')\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # encode src sentences\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # normalise\n",
        "    !cat $DIRECTORY/data/tmp_norm.sp.src.tmp | fairseq-interactive $DIRECTORY/data/data_norm_bin_4000 --source-lang src --target-lang trg --path $DIRECTORY/models/lstm_dict4000_3l_embed384/checkpoint_best.pt > $DIRECTORY/data/tmp_norm.sp.src.output  #2> $DIRECTORY/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/tmp_norm.sp.src.output'))\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc\n",
        "\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_4000.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GDE2DeHKW9g"
      },
      "source": [
        "Prepare the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz0XgzDOKW9g"
      },
      "outputs": [],
      "source": [
        "#getting the test files\n",
        "test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))\n",
        "test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))\n",
        "#normalising the test.src\n",
        "test_norm = normalise(test_src)\n",
        "#save the test.src\n",
        "write_file(test_norm, os.path.join(DIRECTORY,'data/test.norm.trg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F86u5uJxKW9g"
      },
      "source": [
        "BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd-TbuyOKW9g"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "#BLEU\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txJqQJO9KW9g"
      },
      "source": [
        "Translation Error Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au2tPqOKKW9g"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL8hlhuIKW9g"
      },
      "source": [
        "Character n-gram F-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SBrJ0w5KW9g"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(test_norm,[test_trg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk5aG1uyKW9g"
      },
      "source": [
        "Word accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ3LtYOxKW9g"
      },
      "outputs": [],
      "source": [
        "#Wacc\n",
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align(test_trg, test_norm)\n",
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}